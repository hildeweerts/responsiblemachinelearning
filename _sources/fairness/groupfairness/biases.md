(biases_problem)=
# Biases as Sources of Unfairness

```{warning}
This section is still under construction.
```

Algorithmic systems are an accumulation of design choices that embed the developers' explicit and implicit value judgements into the system {footcite:p}`Wieringa2020`. Consequently, biases can seep into the system in many different places of the development process. In this section, we will explore biases as sources of unfairness in different parts of the machine learning development process. 

But first, let us define more precisely what we mean by bias. Generally speaking, {term}`bias` is a systematic and disproportionate tendency towards something. The word bias can be used to refer to many different things, ranging from social biases related to prejudice, to statistical biases that are of a technical nature.

* **Social and systemic bias.** In everyday language, bias often considers prejudice against a person or a group, which we will refer to as {term}`social bias`. Social bias is a form of {term}`cognitive bias`: a systematic error in rational thinking that can affect judgment and decision-making. Most {term}`cognitive bias`es are a result of the limitations of information-processing capabilities of the human brain. From an evolutionary perspective, these biases are useful because they allow people to make quick decisions in critical scenarios. As a hunter-gatherer, you would probably rather be safe than sorry when encountering an unknown group of other humans. However, these shortcuts often come at the cost of the quality of decisions. In particular, stereotypes formed by social bias can be overgeneralized and inaccurate, especially on an individual level. When people act on social biases, they can result in discriminatory practices. It is important to realize that everybody has some degree of conscious or subconscious social bias. Awareness of {term}`cognitive bias`es, including social bias, can help to signal and mitigate their effects. In particular, a diverse team and critical self-reflection can help to signal social biases and avoid acting on them.

* **Statistical bias.** In statistics, bias refers to a systematic error in the estimation of parameters or variables. {term}`statistical bias` can be the result of data collection practices that compromise the accuracy of the estimate, such as a particular sampling procedure. This form of statistical bias can be rooted in {term}`cognitive bias`es of the researcher or data subjects. Statistical bias can also refer to systematic errors caused by assumptions of the estimator. In the context of machine learning algorithms, this type of bias is often discussed in relation to the bias-variance trade-off. For example, some machine learning algorithms can only learn linear relationships between features, whereas the true underlying data distribution exhibits more complex relationships, resulting in an {term}`underfitting` model.

Although all these different types of bias can result in fairness related harms, most issues arise at the intersection of social bias and statistical bias. In the remainder of this section, we will dive more deeply into different types of biases at each stage of the development process. Note that this list is not exhaustive. Moreover, as we will see, biases are hard to precisely dissect and often overlap. In practice, it is usually difficult if not impossible to know exactly which biases are at play. Luckily, a thoughtful development process that leaves room for self-reflection goes a long way in mitigating harms.

## Problem Understanding: Abstraction Traps
Translating a real-world problem into a machine learning task can be difficult. By definition, a model is a simplification of reality. A data scientist's task is to decide which elements of the real world need to be included in the model and which elements will be left out of scope. To this end, some amount of abstraction is required. This involves removing details to focus attention on general patterns. The impact of your system, both positive and negative, highly depends on how you define the machine learning task. By abstracting away the context surrounding a model and its inputs and outputs, you may accidentally abstract away some of the consequences as well.

A mismatch between the machine learning task and the real-world context is referred to as an {term}`abstraction trap` {footcite:p}`Selbst2019`. Abstraction traps can amplify harmful consequences of your system, particularly those related to fairness.

### The Framing Trap
Machine learning models hardly ever operate in isolation. A decision-making process may incorporate (multiple) other machine learning models or human decision-makers. The {term}`framing trap` considers the failure to model the relevant aspects of the larger system your machine learning model is a part of. 

For example, consider a scenario in which judges need to decide whether a defendant is detained. To assist them in their decision making process, they may be provided with a machine learning model that predicts the risk of recidivism; i.e., the risk that the defendant will re-offend. Notably, the final decision of the judge determines the real-world consequences, not the model's prediction. Hence, if fairness is a requirement, it is not sufficient to consider the output of the model; you also need to consider how the predictions are used by the judges. 

In many real-world systems, several machine learning models are deployed at the same time at different points in the decision-making process. Unfortunately, a system of components that seem fair in isolation do not automatically imply a fair system, i.e. \textit{Fair + Fair $\neq$ Fair} {footcite:p}`Dwork2020`.

To avoid the framing trap, we need to ensure that the way we frame our problem and evaluate our solution includes all relevant components and actors of the sociotechnical system.

### The Portability Trap
A system that is carefully designed for a particular context cannot always be directly applied in a different context. Taking an existing solution and applying it in a different situation without taking into account the differences between the two contexts is known as the {term}`portability trap`. A shift in domain, geographical location, time, or even the nature of the decision-making process all impact the suitability of a system. For example, a voice recognition system optimized for speakers with an Australian accent may fail horribly when deployed in the United States. Similarly, the expectations of a good manager have changed considerably in the past few decades, including a stronger need for soft skills. A model trained on annual reviews in the 1960's will likely not be suitable to make predictions for current managers. The {term}`portability trap` goes beyond performance issues due to differences in the data distribution. It also considers differences in social norms and actors. For example, a chat bot optimized to formulate snarky replies may be considered funny on a gaming platform, but inappropriate or even offensive in a more formal context, such as a website for loan applications. To avoid falling into the {term}`portability trap`, we need to consider whether our problem understanding adequately models the social and technical requirements of the actual deployment context.

### The Formalism Trap
In order to use machine learning, you need to formulate your problem in a way that a mathematical algorithm can understand. This is not a straightforward task: there are usually many different ways to measure something. Some may be more appropriate than others. You fall into the {term}`formalism trap` when your formalization does not adequately take into account the context in which your model will be used. For example, machine learning problem formulations often simplify the decision space to a very limited set of actions {footcite:p}`Mitchell2018`. In lending, the decision space of the machine learning model may consist of two options: reject or accept. In reality, there may be many more actions available, such as recommending a different type of loan.

The {term}`formalism trap` is closely related to the statistical concept of {term}`construct validity`: how well does the formalization measure the construct of interest? Business objectives often involve constructs such as "employee quality" or "creditworthiness" that cannot be measured directly {footcite:p}`Jacobs2019`. In such cases, data scientists may use a {term}`proxy variable` instead. Every variable in a data set is the result of a decision on how a particular construct can be measured into a computer readable scale. For example, Netflix has chosen to measure a viewers' quality judgments with likes, rather than the more commonly used 1 - 5 star rating {footcite:p}`Dobbe2018`.  

Not all variables measure the intended construct equally well. {term}`construct validity bias` is a statistical bias that occurs when a variable does not accurately measure the construct it is supposed to measure. For example, income measures the construct socioeconomic status to some degree, but does not capture other factors such as wealth and education {footcite:p}`Jacobs2019`.

A mismatch between the choice of target variable and the actual construct of interest can be detrimental towards fairness goals. In particular, fairness concerns can arise when the measurement error introduced by the choice of formalization differs across groups. For example, you may be interested in predicting crime, but only have access to a subset of all criminal activity: arrest records. In societies where arrest records are the result of racially biased policing practices, the measurement error will differ across racial groups. Similarly, {footcite:t}`obermeyer2019` found that due to unequal access to healthcare, historically less money has been spent on caring for African-American patients compared to Caucasian patients. Consequently, a system that used healthcare costs as a proxy for true healthcare needs systematically underestimated the needs of African-American patients.

Issues of {term}`construct validity` are especially complex for social constructs such as race and gender. Almost paradoxically, measuring {term}`sensitive characteristic` can introduce bias. In industry and academia, it is common to "infer" these characteristics from observed data, such as facial analysis {footcite:p}`Jacobs2019`. This can be problematic because such approaches often fail to acknowledge that social constructs are inherently contextual, may change over time, and are multidimensional. For example, when talking about race, one may be referring to somebody's racial identity (i.e., self-identified race), their observed race (i.e., the race others believe them to be), their phenotype (i.e., racial appearance), or even their reflected race (i.e., the race they believe others assume them to be). Which dimension you measure will influence the conclusions you can draw (see {footcite:t}`Hanna2020` for a more detailed account).

To avoid falling into the {term}`formalism trap`, data scientists should take into account whether the problem formulation handles understandings of (social) constructs in a way that matches the intended deployment context. To mitigate {term}`construct validity bias`, ideally multiple measures are collected, especially for complex constructs. As we will see, many of the biases that can occur in later steps of the development process can be traced back to the problem of {term}`construct validity`.

### The Ripple Effect Trap
Introducing a machine learning model in a social context may affect the behavior of other actors in the system and, as a result, the context itself. This is known as the {term}`ripple effect trap`. There are several ways in which a social context may change due to the introduction of a new technology. First, the introduction of a new technology might be used to argue for or reinforce power, which can change an organization's dynamics. For example, management may purchase software for monitoring workers, reinforcing the power relationship between management and subordinates. Second, the introduction of a prediction systems may cause reactivity behavior. For example, people might attempt to game an automated loan approval system by dishonestly filling out their data in the hope of a more favorable outcome. Third, a system that was developed for a particular use case, may be used in unintended, perhaps even adversarial, ways. To avoid falling into the {term}`ripple effect trap`, it is important to consider whether the envisioned system changes the context in a predictable way.

### The Solutionism Trap
The possible benefits that machine learning solutions can bring can be very exciting. Unfortunately, machine learning is not the answer to everything (\textit{what?!}). The belief that every problem has a technological solution is referred to as {term}`solutionism`. We fall into the {term}`solutionism trap` when we fail to recognize the machine learning is not the right tool for the problem at hand.

The {term}`solutionism trap` is closely related to the {term}`optimism bias`. This is a {term}`cognitive bias` that causes people to overestimate the likelihood of positive events and underestimate the likelihood of negative events. In the context of algorithmic systems, {term}`optimism bias` occurs when policy makers or developers are overly optimistic about a system's benefits, while underestimating its limitations and weaknesses. In particular, people might overestimate the objectiveness of data and algorithmic systems. If this happens, the system's goals, development, and outcomes might not be sufficiently scrutinized, which can result in systematic harms. 

There are several reasons why machine learning may not be the right tool to solve a problem. In some scenarios, it may not be possible to adequately model the context using automated data collection. For example, consider eligibility for social welfare benefits in the Netherlands. Although the criteria for eligibility are set in the law, some variables, e.g. living situation, are difficult to measure quantitatively. Moreover, the Dutch legal system contains the possibility to deviate from the criteria due to compelling personal circumstances. It is impossible to anticipate all context-dependent situations in advance. As a result, machine learning may not be the best tool for this job. In other scenarios, machine learning may be inappropriate because it lacks human connection. For example, consider a person who is hospitalized. In theory, it may be possible to develop a robot nurse who is perfectly capable of performing tasks such as inserting an IV or washing the patient. However, the patient may also value the genuine interest and concern of a nurse  -- in other words, a human connection, something a machine learning model cannot (or even should not) provide. Furthermore, there may be cases where machine learning is simply overkill. For example, you may wonder whether spending several months on optimizing a deep learning computer vision system to predict the dimensions of items in your online shop is a better approach than simply asking the person who puts the item on the website to fill out the dimensions.  

To avoid falling into the {term}`solutionism trap`, it is useful to consider machine learning as a means to an end. In other words, rather than asking "can we use machine learning", ask, "how can we solve this problem?" and then consider machine learning as one of the options.

## Data Collection and Processing
It may not come as a surprise that many fairness issues arise through biases in data collection and analysis. We will discuss three forms of bias that may be present in the data: {term}`historical bias`, {term}`representation bias`, {term}`measurement bias`.

### Historical Bias
Social biases can be encoded in data. If not accounted for, a machine learning model will reproduce these biases, resulting in unfair outcomes. Generally speaking, {term}`historical bias` comes in two flavors. 

Firstly, {term}`historical bias` can arise due to social biases in human decision-making. This type of bias is particularly prevalent when target labels are based on human judgement. For example, if in the past more men have been hired than women, a model trained on historical decisions will likely reproduce this association. As discussed in the previous section, simply removing the {term}`sensitive feature` is not likely to remove this type of bias due to associations between features. Note that this type of {term}`historical bias` is a form of {term}`construct validity bias`: the historical hiring decisions are a biased proxy for actual suitability of the applicant. Similarly, inaccurate stereotypes can be embedded in texts, images, and annotations produced by people, resulting in systems that reinforce these stereotypes.

A second type of {term}`historical bias`  occurs when the data is a good representation of reality, but reality is biased. In our hiring example, the observed bias could be caused by actual differences in suitability, but these differences are in turn caused by structural inequalities in society. For example, people from lower socioeconomic backgrounds may have had fewer opportunities to get good education, making them less suitable for jobs where such education is required for job performance. Similarly, some stereotypes are accurate at an aggregate level (even if they can be very inaccurate at an individual level!). For example, in many societies female nurses still greatly outnumber male nurses.

Depending on your worldview, you might have a different definition of what is fair in each of these scenarios. In practice, it is usually impossible to distinguish between these two types of {term}`historical bias` from observed data alone. Moreover, they can also occur simultaneously. Consequently, understanding {term}`historical bias` and identifying a mitigation approach that is in line with your own moral values requires a deep understanding of the social context.

### Representation Bias
{term}`representation bias` occurs when some groups are underrepresented in the data {footcite:p}`suresh2020`. A machine learning model might not generalize well for underrepresented groups, causing {term}`quality-of-service harm`. {term}`Representation bias` is especially risky when the data distribution of minority groups differs substantially from the majority group (see also {term}`aggregation bias`). A well-known example of {term}`representation bias` was uncovered by {footcite:t}`buolamwini2018`. They found that the data sets that were used to train commercial facial recognition systems contained predominantly images of white men. Consequently, the models did not generalize well to people with dark skin, especially women.

{term}`Representation bias` is closely related to {term}`selection bias`, a statistical bias that occurs when the data collection or selection results in a non-random sample of the population. If not taken into account, conclusions regarding the studied effect may be wrong. For example, young healthy people may be more likely to volunteer for a vaccine trial than less healthy older people. As a result, conclusions about the side effects may not be representative for the whole population. Notably, {term}`representation bias` can occur even when a sample is truly random, as there may not be sufficient information available for minority groups. Moreover, {term}`representation bias` can be an issue in both training and testing data.

An underlying cause of {term}`representation bias` are blind spots of the collectors. For example, a data science team that consists solely of women is less likely to notice that men are not well represented in the data than a more diverse team. Additionally, some data is easier to get than others. For example, collecting data on the interests of young adults, who often spend hours each day scrolling through their social media feeds, is much easier compared to the interests elderly people who are generally not as active online. 

{term}`representation bias` is relatively easy to solve by getting more data. Additionally, data scientists can leverage techniques that were designed to deal with sampling errors, such as weighting instances. However, these approaches first require you to identify cases in which {term}`representation bias`may occur. Therefore, a more durable solution is to invest in a diverse and inclusive development approach, which will help to avoid leaving groups out of the picture in the first place.

### Measurement and Selection Bias
{term}`measurement bias` is a statistical bias that occurs when data contains systematic errors, due to data collection practices. {term}`measurement bias` can be a cause of {term}`construct validity bias`. If systematic errors are correlated to {term}`sensitive characteristics`, {term}`measurement bias` can become a source of unfairness.

{term}`measurement bias` can occur when the method of observation results in systematic errors. First of all, the measurement process may be different across groups {footcite:p}`suresh2020`, due to a combination of social bias and {term}`confirmation bias`. {term}`confirmation bias` is a {term}`cognitive bias` that refers to people's tendency to look for evidence of existing beliefs and disregard evidence that goes against it. In the context of data analysis, {term}`confirmationbias` can lead to cherry picking data that support a conclusion. As the famous quote by Ronald Coase says: *"if you torture the data long enough, it will confess to anything"* For example, a fraud analyst might overly scrutinize some groups over others. Higher rates of testing will result in more positives, confirming the analysts biased beliefs and skewing the observed base rates. If not accounted for, these skewed numbers will be reproduced by the machine learning model.

Another example of {term}`measurement bias` occurs when different observers interpret reality differently. In medical studies, for example, clinicians might arbitrarily round blood pressure readings up or down to the nearest whole number, depending on what they expect to see. In a machine learning context, this type of bias can occur when annotations reflect social biases, such as stereotypes, of the annotators or decision-makers.

{term}`Measurement bias` can also occur at the side of the data subject. Data subjects might behave differently because they are being observed, especially when data is collected from memory or through self-reporting. Survey responses may be incomplete or inconsistent because participants try to present themselves in a way that is socially desirable. For example, consider self-reported height, scraped from a dating website. In many cultures, tallness is seen as an attractive trait in men. Consequently, men may have exaggerated their height to appear more attractive, resulting in {term}`measurement bias`. Note that this is another example of how {term}`measurement bias` can be a threat to {term}`construct validity`.

{term}`measurement bias` can be mitigated by high-quality data collection procedures. Note that it is not possible to identify cases of {term}`measurement bias` from observational data alone. For example, we cannot know the true underlying fraud rate if we only take into account data produced by a biased fraud detection approach. This highlights the importance of documenting the data collection procedure. % This does not only hold for data that was collected within your organization. Take into account possible {term}`measurement bias` when considering external data sources as well.

## Modeling
Building a machine learning model includes many different choices, ranging from the class of machine learning algorithms that is considered to their hyperparameter settings. Different models may have different consequences related to fairness, depending on the task at hand.

### Aggregation bias
{term}`aggregationbias` occurs when a single model is used for groups that have distinct data distributions {footcite:p}`suresh2020`. If not accounted for, it may lead to a model that does not work well for any of the subgroups. For example, it is known that the relationship between hemoglobin levels (HbA1c) and blood glucose levels differs greatly across genders and ethnicities {footcite:p}`suresh2020`. If these differences are not taken into account, a model that only uses a single interpretation of HbA1c will likely not work well for any of these groups. In combination with {term}`representationbias`, it can lead to a model that only works well for the majority population. 

{term}`aggregation bias` is related to the problem of {term}`underfitting`. Machine learning can be seen as a compression problem that produces a mapping between input features and output variable. Some information is inherently lost because of the chosen mapping {footcite}`Dobbe2018`. In particular, some model classes may not be able to adequately capture the different data distributions. Such an oversimplified model may come at the cost of predictive performance for minority groups, resulting in {term}`quality-of-service harm`.

### Omitted variable bias
{term}`Omitted variable bias` is a statistical bias which occurs when one or more relevant features are left out of a linear regression model. Consequently, the model attributes the effects of the missing feature(s) to the included features, obscuring their true effects. {term}`Omitted variable bias` was originally introduced in the context of statistical models that are used for causal inference. For example, consider a regression-based test for discrimination in loan applications. Imagine that loan officers consider payment history in their decision and that payment history correlates with race. If payment history is not recorded in the data, the results of the regression will attribute the effect of payment history to race, suggesting {term}`direct discrimination` and potentially {term}`procedural harm` when there is not {footcite:p}`jung2019omitted`. This bias can also be relevant for prediction tasks. In particular, excluding {term}`sensitive feature`s from the data may obscure a model's indirect dependence on this feature, attributing their effects to other related features. This makes it more difficult to detect and account for existing {term}`historical bias`.
 
## Evaluation
During the evaluation stage, the final model is scrutinized in more detail. {term}`evaluation bias` refers to the use of performance metrics and procedures that are not appropriate for the way in which the model will be used {footcite:p}`suresh2020`. {footcite:t}`Mitchell2018` identify several underlying assumptions of performance metrics. First, these metrics assume that individual decisions are independent of each other. Note how this assumption is grounded in utilitarianism, in which overall utility is expressed as the sum of individual utilities. In practice, however, the impact of a decision may not be independent across instances. For example, denying one family member a loan may impact another family member's ability to repay their own loan. Additionally, it is typically assumed that decisions are symmetrical, i.e. the impact of the outcome is equal across instances. Again, this often does not hold in practice. For example, a rejection of a job application can have a very different impact depending on whether that person is currently employed or unemployed.

## Deployment
Once the system is deployed, it may be used, interpreted, or interacted with inappropriately, resulting in unfair outcomes {footcite:p}`Friedman1996`. The underlying cause of these outcomes is a mismatch between the system's design and the context in which it will be applied. Indeed, biases in deployment can often be attributed to abstraction traps.

### Usage
The system may be used in a context for which it was not (properly) designed, in which case we fall into the {term}`portability trap`. For example, a toxic language detection model trained on tweets may not be suitable for a platform such as TikTok, where the average user is much younger and may use different language (tone, words, etc.) than an average Twitter user. Note that this type of bias can also accrue over time due to changing populations and behaviors, in which case it can be seen as a form of concept drift.

### Interpretation
Interaction of stakeholders with the system can be a source of unfairness. A decision-maker may interpret the model's output differently for different groups, due to social bias and {term}`confirmation bias`. For example, a judge may weigh a high risk score more heavily for a black defendant compared to a white defendant, due to (unconscious) social bias. This bias, which can be attributed to falling into the framing trap, can be mitigated by taking into account stakeholder interactions during the system's design and evaluation.

### Interaction: Reinforcing Feedback Loops
In systems that learn from user interactions, users can introduce social bias. For example, consider a chat bot that learns dynamically. Without safeguards against toxicity, users might teach it to use obscene or otherwise offensive language, resulting in {term}`denigration harm`. This type of bias can be avoided by putting checks in place to identify malicious intent towards the system.

Feedback mechanisms that amplify an effect are called {term}`reinforcing feedback loop`s. In the context of fairness, it refers to the amplification of existing (historical) biases when new data is collected based on the output of a biased model.

```{admonition} *Example:* A Reinforcing Feedback Loop in Predictive Policing

Lets imagine there is a police station that is responsible for two neighborhoods, $A$ and $B$. Now lets imagine a predictive policing system that allocates police officers to the neighborhoods based on the predicted crime rate in each neighborhood. In this example, the true crime rates of the neighborhoods are equal. However, due to the randomness, we have collected slightly more crime data in neighborhood $A$ than than in neighborhood $B$ at the time the prediction model is trained. Consequently, the model predicts more crime in neighborhood $A$ than in neighborhood $B$. Based on this prediction, we send more police officers to neighborhood $A$. Consequently, more crime will be detected in neighborhood $A$ -- even though the true crime rates are the same. If we retrain our model on the newly collected crime data, even more police officers will be allocated to neighborhood $A$ and even more crime is detected. And so the feedback loop continues... 
```

A consequence of these feedback loops is that people can form erroneous beliefs based on the data. For example, after the introduction of the predictive policing system in our example, police officers may believe that neighborhood $A$ truly has a bigger crime problem than neighborhood $B$. A failure to anticipate on feedback loops can be particularly risky for automated decision-making systems, in which bias can propagate quickly over time.

<!-- A specific instance of feedback loops that recommender systems may suffer from is {term}`popularity bias`. If people tend to click on highly ranked items more often, this can lead the algorithm to rank popular items even higher and disregard less popular items that may be just as valuable to the user. 
One way to investigate feedback loops is through simulation. Developing an accurate simulation of a sociotechnical system is difficult and requires a lot of domain expertise. Alternatively, we may borrow approaches from the field of system dynamics {footcite:p}`martin2020extending` and causal modeling. -->

```{footbibliography}
```