@misc{compas2017,
title = {{Machine Bias}},
author = {Julia Angwin and Jeff Larson},
year = {2016},
url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
}

@misc{intelligence2020assessment,
author = {{Independent Expert Group on Artificial Intelligence}},
pages = {0--33},
title = {{The Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self assessment}},
url = {https://ec.europa.eu/newsroom/dae/document.cfm?doc_id=68342},
year = {2020}
}

@inproceedings{buolamwini2018,
  title={Gender shades: Intersectional accuracy disparities in commercial gender classification},
  author={Buolamwini, Joy and Gebru, Timnit},
  booktitle={Conference on fairness, accountability and transparency},
  pages={77--91},
  year={2018}
}

@inproceedings{Wieringa2020,
  doi = {10.1145/3351095.3372833},
  url = {https://doi.org/10.1145/3351095.3372833},
  year = {2020},
  month = jan,
  publisher = {{ACM}},
  author = {Maranke Wieringa},
  title = {What to account for when accounting for algorithms},
  booktitle = {Proceedings of the 2020 Conference on Fairness,  Accountability,  and Transparency}
}

@article{Friedman1996,
  doi = {10.1145/230538.230561},
  url = {https://doi.org/10.1145/230538.230561},
  year = {1996},
  month = jul,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {14},
  number = {3},
  pages = {330--347},
  author = {Batya Friedman and Helen Nissenbaum},
  title = {Bias in computer systems},
  journal = {{ACM} Transactions on Information Systems}
}

@article{Madaio2020,
abstract = {Many organizations have published principles intended to guide the ethical development and deployment of AI systems; however, their abstract nature makes them difficult to oper-ationalize. Some organizations have therefore produced AI ethics checklists, as well as checklists for more specific concepts , such as fairness, as applied to AI systems. But unless checklists are grounded in practitioners' needs, they may be misused. To understand the role of checklists in AI ethics, we conducted an iterative co-design process with 48 practitioners, focusing on fairness. We co-designed an AI fairness checklist and identified desiderata and concerns for AI fairness checklists in general. We found that AI fairness checklists could provide organizational infrastructure for formalizing ad-hoc processes and empowering individual advocates. We discuss aspects of organizational culture that may impact the efficacy of such checklists, and highlight future research directions.},
author = {Madaio, Michael A and Stark, Luke and {Wortman Vaughan}, Jennifer and Wallach, Hanna},
file = {:Users/hildeweerts/Desktop/TUe/{\_}04 Papers/checklists.pdf:pdf},
isbn = {9781450367080},
journal = {Chi 2020},
keywords = {Author Keywords AI,ML,checklists CCS Concepts •Human-centered computing,co-design,ethics,fairness,•Computing methodologies ! Machine learning,•Social and professional topics ! Codes of ethics},
mendeley-groups = {01 Fairness/Tools},
pages = {1--14},
title = {{Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI}},
year = {2020}
}

@article{Guo2019,
  author    = {Anhong Guo and
               Ece Kamar and
               Jennifer Wortman Vaughan and
               Hanna M. Wallach and
               Meredith Ringel Morris},
  title     = {Toward Fairness in {AI} for People with Disabilities: {A} Research
               Roadmap},
  journal   = {CoRR},
  volume    = {abs/1907.02227},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.02227},
  archivePrefix = {arXiv},
  eprint    = {1907.02227},
  timestamp = {Mon, 08 Jul 2019 14:12:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-02227.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Rudin2018a,
journal = {Harvard Data Science Review},
doi = {10.1162/99608f92.6ed64b30},
number = {1},
note = {https://hdsr.mitpress.mit.edu/pub/7z10o269},
publisher = {},
title = {The Age of Secrecy and Unfairness in Recidivism Prediction},
url = {https://hdsr.mitpress.mit.edu/pub/7z10o269},
volume = {2},
author = {Rudin, Cynthia and Wang, Caroline and Coker, Beau},
date = {2020-03-31},
year = {2020},
month = {3},
day = {31},
}

@article{lipton2019troubling, 
author = {Lipton, Zachary C. and Steinhardt, Jacob}, 
title = {Troubling Trends in Machine Learning Scholarship: Some ML Papers Suffer from Flaws That Could Mislead the Public and Stymie Future Research.}, 
year = {2019}, 
issue_date = {January-February 2019}, 
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}, 
volume = {17}, 
number = {1}, 
issn = {1542-7730}, 
url = {https://doi.org/10.1145/3317287.3328534}, 
doi = {10.1145/3317287.3328534}, 
abstract = {Flawed scholarship threatens to mislead the public and stymie future research by compromising ML’s intellectual foundations. 
Indeed, many of these problems have recurred cyclically throughout the history of AI and, more broadly, in scientific research. In 1976, 
Drew McDermott chastised the AI community for abandoning self-discipline, warning prophetically that "if we can’t criticize ourselves, 
someone else will save us the trouble." The current strength of machine learning owes to a large body of rigorous research to date, 
both theoretical and empirical. By promoting clear scientific thinking and communication, our community can sustain the trust and 
investment it currently enjoys.}, 
journal = {Queue}, 
month = {feb},
pages = {45–77}, 
numpages = {33} 
}

@article{Kamiran2013,
abstract = {Recently the following discrimination-aware classification problem was introduced. Historical data used for supervised learning may contain discrimina- tion; for instance, with respect to gender. The question addressed by discrimination- aware techniques is, given sensitive attribute, how to train discrimination-free classifiers on such historical data that is discriminative, with respect to the given sensitive attribute. Existing techniques that deal with this problem aim at remov- ing all discrimination and do not take into account that part of the discrimination may be explainable by other attributes. For example, in a job application, the education level of a job candidate could be such an explainable attribute. If the data containsmany highly-educatedmale candidates and only few highly-educated woman, a difference in acceptance rates between woman and man does not neces- sarily reflect gender discrimination, as it could be explained by the different levels of education. Even though selecting on education level would result in more males being accepted, a difference with respect to such a criterion would not be consid- ered to be undesirable, nor illegal. Current state-of-the-art techniques, however, do not take such gender-neutral explanations into account, and tend to overreact and actually start reverse discriminating, as we will show in this paper. Therefore, we introduce and analyze the refined notion of conditional non-discrimination in classifier design. We show that some of the differences in decisions across the sen- sitive groups can be explainable and are hence tolerable. Therefore, we develop methodology for quantifying the explainable discrimination and algorithmic tech- niques for removing the illegal discrimination when one or more attributes are considered as explanatory. Experimental evaluation on synthetic and real world classification datasets demonstrate that the new techniques are superior to the old ones in this new context, as they succeed in removing almost exclusively the undesirable discrimination, while leaving the explainable differences unchanged, allowing for differences in decisions as long as they are explainable.},
author = {Kamiran, Faisal and \v{Z}liobait\.{e}, Indr\.{e} and Calders, Toon},
journal = {Knowledge and Information Systems},
doi = {10.1007/s10115-012-0584-8},
file = {:Users/hildeweerts/Downloads/724108292794707.pdf:pdf},
isbn = {1011501205848},
issn = {02191377},
keywords = {Classification,Discrimination-aware data mining,Independence},
number = {3},
pages = {613--644},
title = {{Quantifying explainable discrimination and removing illegal discrimination in automated decision making}},
volume = {35},
year = {2013}
}

@article{wachter2021fairness,
title = {Why fairness cannot be automated: Bridging the gap between EU non-discrimination law and AI},
journal = {Computer Law & Security Review},
volume = {41},
pages = {105567},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105567},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000406},
author = {Sandra Wachter and Brent Mittelstadt and Chris Russell},
keywords = {European union, Non-discrimination, Fairness, Discrimination, Bias, Algorithm, Law, Demographic parity, Machine learning, Artificial intelligence},
abstract = {In recent years a substantial literature has emerged concerning bias, discrimination, and fairness in artificial intelligence (AI) and machine learning. Connecting this work to existing legal non-discrimination frameworks is essential to create tools and methods that are practically useful across divergent legal regimes. While much work has been undertaken from an American legal perspective, comparatively little has mapped the effects and requirements of EU law. This Article addresses this critical gap between legal, technical, and organisational notions of algorithmic fairness. Through analysis of EU non-discrimination law and jurisprudence of the European Court of Justice (ECJ) and national courts, we identify a critical incompatibility between European notions of discrimination and existing work on algorithmic and automated fairness. A clear gap exists between statistical measures of fairness as embedded in myriad fairness toolkits and governance mechanisms and the context-sensitive, often intuitive and ambiguous discrimination metrics and evidential requirements used by the ECJ; we refer to this approach as “contextual equality.” This Article makes three contributions. First, we review the evidential requirements to bring a claim under EU non-discrimination law. Due to the disparate nature of algorithmic and human discrimination, the EU's current requirements are too contextual, reliant on intuition, and open to judicial interpretation to be automated. Many of the concepts fundamental to bringing a claim, such as the composition of the disadvantaged and advantaged group, the severity and type of harm suffered, and requirements for the relevance and admissibility of evidence, require normative or political choices to be made by the judiciary on a case-by-case basis. We show that automating fairness or non-discrimination in Europe may be impossible because the law, by design, does not provide a static or homogenous framework suited to testing for discrimination in AI systems. Second, we show how the legal protection offered by non-discrimination law is challenged when AI, not humans, discriminate. Humans discriminate due to negative attitudes (e.g. stereotypes, prejudice) and unintentional biases (e.g. organisational practices or internalised stereotypes) which can act as a signal to victims that discrimination has occurred. Equivalent signalling mechanisms and agency do not exist in algorithmic systems. Compared to traditional forms of discrimination, automated discrimination is more abstract and unintuitive, subtle, intangible, and difficult to detect. The increasing use of algorithms disrupts traditional legal remedies and procedures for detection, investigation, prevention, and correction of discrimination which have predominantly relied upon intuition. Consistent assessment procedures that define a common standard for statistical evidence to detect and assess prima facie automated discrimination are urgently needed to support judges, regulators, system controllers and developers, and claimants. Finally, we examine how existing work on fairness in machine learning lines up with procedures for assessing cases under EU non-discrimination law. A ‘gold standard’ for assessment of prima facie discrimination has been advanced by the European Court of Justice but not yet translated into standard assessment procedures for automated discrimination. We propose ‘conditional demographic disparity’ (CDD) as a standard baseline statistical measurement that aligns with the Court's ‘gold standard’. Establishing a standard set of statistical evidence for automated discrimination cases can help ensure consistent procedures for assessment, but not judicial interpretation, of cases involving AI and automated systems. Through this proposal for procedural regularity in the identification and assessment of automated discrimination, we clarify how to build considerations of fairness into automated systems as far as possible while still respecting and enabling the contextual approach to judicial interpretation practiced under EU non-discrimination law.}
}


@inproceedings{Kusner2017,
abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
archivePrefix = {arXiv},
arxivId = {1703.06856},
author = {Kusner, Matt and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
booktitle = {Advances in Neural Information Processing Systems 30},
eprint = {1703.06856},
file = {:Users/hildeweerts/Desktop/TUe/{\_}04 Papers/fairness/6995-counterfactual-fairness.pdf:pdf},
issn = {10495258},
mendeley-groups = {01 Fairness/Counterfactual Fairness},
title = {{Counterfactual fairness}},
year = {2017}
}

@misc{fairmlbook,
author = {Barocas, Solon and Hardt, Moritz and Narayanan, Menaka},
title = {{Fairness and machine learning: limitations and opportunities}},
year = {2022},
Note = {Retrieved from \url{https://fairmlbook.org}}
}

@misc{goorbergh2022harm,
  doi = {10.48550/ARXIV.2202.09101},
  url = {https://arxiv.org/abs/2202.09101},
  author = {Goorbergh, Ruben van den and van Smeden, Maarten and Timmerman, Dirk and Van Calster, Ben},
  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {The harm of class imbalance corrections for risk prediction models: illustration and simulation using logistic regression},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@article{Selbst2019,
abstract = {A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science-such as abstraction and modular design-are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce "fair" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five "traps" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.},
author = {Selbst, Andrew D. and Boyd, Danah and Friedler, Sorelle A. and Venkatasubramanian, Suresh and Vertesi, Janet},
doi = {10.1145/3287560.3287598},
file = {:Users/hildeweerts/Desktop/TUe/{\_}04 Papers/fairness/selbst-et-al-fairness-and-abstraction-in-sociotechnical-systems.pdf:pdf},
isbn = {9781450361255},
journal = {FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency},
keywords = {Fairness-aware Machine Learning,Interdisciplinary,Sociotechnical Systems},
mendeley-groups = {01 Fairness/Reviews,01 Fairness/Social sciences},
pages = {59--68},
title = {{Fairness and abstraction in sociotechnical systems}},
year = {2019}
}

@inproceedings{Dwork2012,
  title={Fairness through awareness},
  author={Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  booktitle={Proceedings of the 3rd innovations in theoretical computer science conference},
  pages={214--226},
  year={2012}
}

@article{Dwork2020,
  title={Individual fairness in pipelines},
  author={Dwork, Cynthia and Ilvento, Christina and Jagadeesan, Meena},
  journal={arXiv preprint arXiv:2004.05167},
  year={2020}
}

@misc{Mitchell2018,
    title={Prediction-Based Decisions and Fairness: A Catalogue of Choices, Assumptions, and Definitions},
    author={Shira Mitchell and Eric Potash and Solon Barocas and Alexander D'Amour and Kristian Lum},
    year={2018},
    eprint={1811.07867},
    archivePrefix={arXiv},
    primaryClass={stat.AP}
}

@inproceedings{Jacobs2019, 
    author = {Jacobs, Abigail Z. and Wallach, Hanna}, title = {Measurement and Fairness}, year = {2021}, isbn = {9781450383097}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3442188.3445901}, doi = {10.1145/3442188.3445901}, abstract = {We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.}, booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency}, pages = {375–385}, numpages = {11}, keywords = {construct reliability, fairness, construct validity, measurement}, location = {Virtual Event, Canada}, series = {FAccT '21} 
}

@article{Dobbe2018,
  title={A broader view on bias in automated decision-making: Reflecting on epistemology and dynamics},
  author={Dobbe, Roel and Dean, Sarah and Gilbert, Thomas and Kohli, Nitin},
  journal={arXiv preprint arXiv:1807.00553},
  year={2018}
}

@article{Obermeyer2019,
	author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
	title = {Dissecting racial bias in an algorithm used to manage the health of populations},
	volume = {366},
	number = {6464},
	pages = {447--453},
	year = {2019},
	doi = {10.1126/science.aax2342},
	publisher = {American Association for the Advancement of Science},
	abstract = {The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care.Science, this issue p. 447; see also p. 421Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/366/6464/447},
	eprint = {https://science.sciencemag.org/content/366/6464/447.full.pdf},
	journal = {Science}
}


@inproceedings{Hanna2020,
abstract = {We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.},
archivePrefix = {arXiv},
arxivId = {1912.03593},
author = {Hanna, Alex and Denton, Emily and Smart, Andrew and Smith-Loud, Jamila},
booktitle = {FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
doi = {10.1145/3351095.3372826},
eprint = {1912.03593},
file = {:Users/hildeweerts/Library/Application Support/Mendeley Desktop/Downloaded/Hanna et al. - 2019 - Towards a Critical Race Methodology in Algorithmic Fairness.pdf:pdf},
isbn = {9781450369367},
keywords = {Algorithmic fairness,Critical race theory,Ethnicity,Race},
mendeley-groups = {01 Fairness,01 Fairness/Social sciences,01 Fairness/Measuring Fairness},
month = {dec},
pages = {501--512},
title = {{Towards a critical race methodology in algorithmic fairness}},
url = {https://arxiv.org/abs/1912.03593},
year = {2020}
}

@misc{suresh2020,
      title={A Framework for Understanding Unintended Consequences of Machine Learning}, 
      author={Harini Suresh and John V. Guttag},
      year={2020},
      eprint={1901.10002},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{martin2020extending,
      title={Extending the Machine Learning Abstraction Boundary: A Complex Systems Approach to Incorporate Societal Context}, 
      author={ Martin, Jr., Donald and Vinodkumar Prabhakaran and Jill Kuhlberg and Andrew Smart and William S. Isaac},
      year={2020},
      eprint={2006.09663},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{jung2019omitted,
      title={Omitted and Included Variable Bias in Tests for Disparate Impact}, 
      author={Jongbin Jung and Sam Corbett-Davies and Ravi Shroff and Sharad Goel},
      year={2019},
      eprint={1809.05651},
      archivePrefix={arXiv},
      primaryClass={stat.AP}
}

@article{hardt2016equality,
  title={Equality of opportunity in supervised learning},
  author={Hardt, Moritz and Price, Eric and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@misc{kleinberg2016inherent,
  doi = {10.48550/ARXIV.1609.05807},
  
  url = {https://arxiv.org/abs/1609.05807},
  
  author = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
  
  keywords = {Machine Learning (cs.LG), Computers and Society (cs.CY), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Inherent Trade-Offs in the Fair Determination of Risk Scores},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{chouldechova2017fair,
  title={Fair prediction with disparate impact: A study of bias in recidivism prediction instruments},
  author={Chouldechova, Alexandra},
  journal={Big data},
  volume={5},
  number={2},
  pages={153--163},
  year={2017},
  publisher={Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA}
}

@article{strumbelj2009explaining,
abstract = {In this paper, we present a novel method for explaining the decisions of an arbitrary classifier, independent of the type of classifier. The method works at the instance level, decomposing the model's prediction for an instance into the contributions of the attributes' values. We use several artificial data sets and several different types of models to show that the generated explanations reflect the decision-making properties of the explained model and approach the concepts behind the data set as the prediction quality of the model increases. The usefulness of the method is justified by a successful application on a real-world breast cancer recurrence prediction problem. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {{\v{S}}trumbelj, E. and Kononenko, I. and {Robnik {\v{S}}ikonja}, M.},
doi = {10.1016/j.datak.2009.01.004},
file = {:Users/hildeweerts/Desktop/TUe/Thesis/03 Papers/Explaining instance classifications with interactions of subsets of feature values - Strumbelj, Kononenko, Robnik-Sikonja (2008).pdf:pdf},
issn = {0169023X},
journal = {Data and Knowledge Engineering},
keywords = {Classification,Data mining,Explanation,Knowledge discovery,Machine learning,Visualization},
mendeley-groups = {Thesis},
number = {10},
pages = {886--904},
publisher = {Elsevier B.V.},
title = {{Explaining instance classifications with interactions of subsets of feature values}},
url = {http://dx.doi.org/10.1016/j.datak.2009.01.004},
volume = {68},
year = {2009}
}

@article{strumbelj2014explaining,
abstract = {We presented a general method for explaining prediction models with contributions of input features. Similar to other general methods, the method is based on sensitivity analysis. However, to deal with the shortcomings of related methods, all subsets of input features are perturbed. The algorithm is simple to implement and can be applied to any type classification or regression model, which simplifies its use and facilitates comparison. When explaining an an additive model, the explanation has the same properties as commonly used additive model-specific methods. Examples from artificial and real-world data sets and a user study illustrate the methods usefulness.},
author = {{\v{S}}trumbelj, Erik and Kononenko, Igor},
doi = {10.1007/s10115-013-0679-x},
isbn = {0219-1377},
issn = {02193116},
journal = {Knowledge and Information Systems},
keywords = {Data mining,Decision support,Interpretability,Knowledge discovery,Visualization},
mendeley-groups = {Thesis},
number = {3},
pages = {647--665},
title = {{Explaining prediction models and individual predictions with feature contributions}},
volume = {41},
year = {2014}
}

@incollection{lundberg2017unified,
author = {Lundberg, Scott M and Lee, Su-In},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
file = {:Users/hildeweerts/Desktop/TUe/Thesis/03 Papers/A Unified Approach to Interpreting Model Predictions - Lundberg (2017).pdf:pdf},
mendeley-groups = {Thesis},
pages = {4765--4774},
publisher = {Curran Associates, Inc.},
title = {{A Unified Approach to Interpreting Model Predictions}},
url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf},
year = {2017}
}

@inproceedings{ribeiro2016why,
address = {New York, New York, USA},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
booktitle = {Proceedings of the 22nd ACM SIG International Conference on Knowledge Discovery and Data Mining (KDD)},
doi = {10.1145/2939672.2939778},
file = {:Users/hildeweerts/Desktop/TUe/Thesis/03 Papers/why should i trust you - explaining the predictions of any classifier - Ribeiro (2016).pdf:pdf},
isbn = {9781450342322},
mendeley-groups = {Thesis},
pages = {1135--1144},
publisher = {ACM Press},
title = {{"Why Should I Trust You?": Explaining the Predictions of Any Classifier}},
url = {http://dl.acm.org/citation.cfm?doid=2939672.2939778},
year = {2016}
}


@article{robnik-sikonja2008explaining,
abstract = {We present a method for explaining predictions for individual instances. The presented approach is general and can be used with all classification models that output probabilities. It is based on the decomposition of a model's predictions on individual contributions of each attribute. Our method works for the so-called black box models such as support vector machines, neural networks, and nearest neighbor algorithms, as well as for ensemble methods such as boosting and random forests. We demonstrate that the generated explanations closely follow the learned models and present a visualization technique that shows the utility of our approach and enables the comparison of different prediction methods.},
author = {Robnik-{\v{S}}ikonja, Marko and Kononenko, Igor},
doi = {10.1109/TKDE.2007.190734},
isbn = {1041-4347},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
number = {5},
pages = {589--600},
title = {{Explaining classifications for individual instances}},
volume = {20},
year = {2008}
}

@incollection{shapley1953,
  title = {A Value for n-Person Games},
  author = {Shapley, Lloyd S},
  booktitle = {Contributions to the Theory of Games II},
  editor = {Kuhn, Harold W. and Tucker, Albert W.},
  pages = {307--317},
  year = {1953},
  publisher = {Princeton University Press},
  address = {Princeton}
}

@article{lipovetsky2001analysis,
  doi = {10.1002/asmb.446},
  url = {https://doi.org/10.1002/asmb.446},
  year  = {2001},
  publisher = {Wiley},
  volume = {17},
  number = {4},
  pages = {319--330},
  author = {Stan Lipovetsky and Michael Conklin},
  title = {Analysis of regression in game theory approach},
  journal = {Applied Stochastic Models in Business and Industry}
}

@inproceedings{raji2022fallacy,
  doi = {10.1145/3531146.3533158},
  url = {https://doi.org/10.1145/3531146.3533158},
  year = {2022},
  month = jun,
  publisher = {{ACM}},
  author = {Inioluwa Deborah Raji and I. Elizabeth Kumar and Aaron Horowitz and Andrew Selbst},
  title = {The Fallacy of {AI} Functionality},
  booktitle = {2022 {ACM} Conference on Fairness,  Accountability,  and Transparency}
}

@misc{paullada2020data,
      title={Data and its (dis)contents: A survey of dataset development and use in machine learning research}, 
      author={Amandalynne Paullada and Inioluwa Deborah Raji and Emily M. Bender and Emily Denton and Alex Hanna},
      year={2020},
      eprint={2012.05345},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{carlini2020extracting,
      title={Extracting Training Data from Large Language Models}, 
      author={Nicholas Carlini and Florian Tramer and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown and Dawn Song and Ulfar Erlingsson and Alina Oprea and Colin Raffel},
      year={2020},
      eprint={2012.07805},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@inproceedings{Raji2020a,
    title = {{Concrete Problems in AI Safety, Revisited}},
    author={I.D. Raji and R. Dobbe},
    year = {2020},
    booktitle = {Workshop on Machine Learning In Real Life at the International Conference on Learning Representations, Addis Abeba, Ethiopia [Online]},
    month = apr,
    url={https://drive.google.com/file/d/1Re_yQDNFuejoqjZloTgQpILosDGtt5ei/view}
}


@article{boyd2012,
  doi = {10.1080/1369118x.2012.678878},
  url = {https://doi.org/10.1080/1369118x.2012.678878},
  year = {2012},
  month = jun,
  publisher = {Informa {UK} Limited},
  volume = {15},
  number = {5},
  pages = {662--679},
  author = {danah boyd and Kate Crawford},
  title = {Critical questions for big data},
  journal = {Information,  Communication {\&} Society}
}

@article{chen2018my,
  title={Why is my classifier discriminatory?},
  author={Chen, Irene and Johansson, Fredrik D and Sontag, David},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{hanna2020towards,
  title={Towards a critical race methodology in algorithmic fairness},
  author={Hanna, Alex and Denton, Emily and Smart, Andrew and Smith-Loud, Jamila},
  booktitle={Proceedings of the 2020 conference on fairness, accountability, and transparency},
  pages={501--512},
  year={2020}
}

@inproceedings{wojcik2023assessing,
    author={Malwina Anna Wójcik},    
    title = {Assessing the Legality of Using the Category of Race and Ethnicity in Clinical Algorithms - the EU Anti-discrimination Law Perspective},
    booktitle = {EWAF'23: European Workshop on Algorithmic Fairness},
    year = {2023},
    date={June 07–09, 2023},
    location= {Winterthur, Switzerland}
}