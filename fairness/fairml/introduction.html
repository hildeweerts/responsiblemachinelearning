
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Fairness-Aware Machine Learning &#8212; An Introduction to Responsible Machine Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Pre-processing Algorithms" href="preprocessing.html" />
    <link rel="prev" title="Group Fairness Metrics" href="../groupfairnessmetrics.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">An Introduction to Responsible Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    An Introduction to Responsible Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/introduction.html">
   Responsible Machine Learning
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../introduction/machinelearning/introduction.html">
   Machine Learning Preliminaries
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../introduction/machinelearning/modelevaluation.html">
     Model Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../introduction/machinelearning/modelselection.html">
     Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../introduction/machinelearning/costsensitivelearning.html">
     Cost-Sensitive Learning
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Fairness
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction.html">
   Algorithmic Fairness
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../groupfairnessmetrics.html">
   Group Fairness Metrics
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="current reference internal" href="#">
   Fairness-Aware Machine Learning
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="preprocessing.html">
     Pre-processing Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="constrainedlearning.html">
     Constrained Learning Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="postprocessing.html">
     Post-processing Algorithms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../interdisciplinary/introduction.html">
   Interdisciplinary Perspectives on Fair-ML
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../interdisciplinary/philosophy.html">
     Philosophy: What is “Fair”?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interdisciplinary/law.html">
     Law: Fairness and Discrimination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../interdisciplinary/sts.html">
     Science and Technology Studies: Abstraction Traps
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../tutorials/introduction.html">
   Tutorials
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../tutorials/measuringgroupfairness.html">
     Measuring Group Fairness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../tutorials/fairnessawareclassification.html">
     Fairness-Aware Classification
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Misc
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../misc/harms.html">
   List of Harms
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/hildeweerts/responsiblemachinelearning"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/hildeweerts/responsiblemachinelearning/issues/new?title=Issue%20on%20page%20%2Ffairness/fairml/introduction.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/fairness/fairml/introduction.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#no-fairness-through-unawareness">
   No Fairness Through Unawareness
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fairness-aware-machine-learning-algorithms">
   Fairness-aware Machine Learning Algorithms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Fairness-Aware Machine Learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#no-fairness-through-unawareness">
   No Fairness Through Unawareness
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fairness-aware-machine-learning-algorithms">
   Fairness-aware Machine Learning Algorithms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="fairness-aware-machine-learning">
<span id="fairml"></span><h1>Fairness-Aware Machine Learning<a class="headerlink" href="#fairness-aware-machine-learning" title="Permalink to this headline">#</a></h1>
<p>So far, we have covered various ways in which machine learning systems can result in unfair outcomes, several biases that lie at the root of fairness-related harm, and a set of metrics that can be used to measure harm. A natural question, then, is: how can we ensure that our machine learning model adheres to a certain fairness constraint? Over the past few years, computer scientists have been productive in developing various algorithms aimed at mitigating unfairness.</p>
<section id="no-fairness-through-unawareness">
<h2>No Fairness Through Unawareness<a class="headerlink" href="#no-fairness-through-unawareness" title="Permalink to this headline">#</a></h2>
<p>Now, you may wonder: if we wish to avoid fairness-related harm, why don’t we just remove the sensitive feature from our data set? Unfortunately, it is not that simple.</p>
<p>While removing a sensitive feature ensures the model cannot explicitly use a sensitive feature as input to a prediction, machine learning models will often still be able to replicate the association between a sensitive feature and the target variable. In its simplest form, one of the remaining features in the data set may act as a proxy variable for a sensitive feature. The use of proxy variables is common in cases of (subconscious and conscious) discrimination by human actors. A classical example is redlining, a practice in the United States where people were systematically denied services based on their postal code. Neighborhoods that were deemed “too risky” were outlined on the map in the color red. Although postal code may appear to be a neutral feature, it was highly correlated with race. As services were mostly denied in predominantly African-American neighborhoods, African Americans were disproportionately affected by this practice. Another example can be found in loan applications. Imagine we want to avoid allocation harm across genders. We can decide to exclude the feature that represents gender from our data set, to avoid any direct discrimination. However, if we do include occupation, an attribute that is highly gendered in many societies, the model can still identify historical patterns of gender bias. Here, occupation can unintentionally act as a proxy variable for gender.</p>
<p>As an increasing number of empirical evidence highlights, this is not just a hypothetical problem <a class="footnote-reference brackets" href="#footcite-kamiran2012data" id="id1">1</a> <a class="footnote-reference brackets" href="#footcite-kamishima2012fairness" id="id2">2</a>. Machine learning algorithms are specifically designed to identify relationships between features in a data set. If undesirable patterns exist within the data, a machine learning model will likely replicate it. Removing all possible proxy variables is usually not a viable approach. First of all, it is not always possible to anticipate the patterns through which the sensitive feature can be approximated by the model. Several features that are slightly predictive of the sensitive feature might, taken together, accurately predict a sensitive feature. Second, apart from their relation with the sensitive feature, proxy variables may provide information that is predictive of the target feature. Removing all features that are slightly related to the sensitive feature could therefore substantially reduce the predictive performance of the model.</p>
<p>Removing sensitive features is unlikely to prevent allocation harm, which can still occur in the form of indirect effects. Similarly, patterns of stereotyping and denigration can be deeply embedded in (unstructured) data such as text. Quality-of-service harm, representation harm, and denigration harm can be caused by a lack of informativeness of the data that is available for these groups, which is not solved by removing the sensitive feature either. To conclude, removing sensitive features is only helpful in achieving an extremely narrow notion of fairness. The practical consequence is that it is unlikely that this approach will prevent real-world harm.</p>
</section>
<section id="fairness-aware-machine-learning-algorithms">
<h2>Fairness-aware Machine Learning Algorithms<a class="headerlink" href="#fairness-aware-machine-learning-algorithms" title="Permalink to this headline">#</a></h2>
<p>A common approach in computer science literature on algorithmic fairness is to formulate unfairness mitigation as an optimization task, to achieve high predictive performance whilst satisfying a fairness constraint. Generally speaking, we can distinguish three types of approaches that intervene in different parts of the machine learning process.</p>
<ul class="simple">
<li><p><strong>Pre-processing algorithms.</strong> Pre-processing algorithms make adjustments to the data to mitigate the unfairness of the downstream machine learning model.
Most pre-processing algorithms are designed to obscure undesirable associations between one or more sensitive features and the target variable of the model.</p></li>
<li><p><strong>Constrained learning algorithms.</strong> Constrained learning techniques directly incorporate a fairness constraint in the learning algorithm, typically by adjusting existing learning paradigms. For example, a typical constrained learning approach adjusts the loss function of an existing machine learning algorithm to penalize unfair predictions. This category also includes wrapper methods, which enforce fairness constraints via optimization external to the machine learning algorithm itself, such as hyperparameter optimization or ensemble learning.</p></li>
<li><p><strong>Post-processing algorithms.</strong> Post-processing approaches adjust an existing model, either through post-processing predictions (e.g., by shifting the decision thresholds) or by adjusting the learned model parameters directly (e.g., the coefficients of a linear regression model or the labels assigned to the leaves of a decision tree.)</p></li>
</ul>
<p>The dividing line between different approaches is not always clear-cut. For example, data resampling straddles between pre-processing and constrained learning approaches, while decision threshold optimization can be implemented as a metric-agnostic hyperparameter search or a post-processing approach that takes into consideration the nature of the fairness constraint that is to be enforced.</p>
<p>In the remainder of this chapter, we will discuss several simple fairness-aware machine learning algorithms. The goal is not to present a comprehensive overview of all the different techniques that have been proposed in the algorithmic fairness literature. Instead, these examples allow you to develop an intuition of the advantages, disadvantages, and limitations of the fairness-aware machine learning paradigm.</p>
<div class="tip admonition">
<p class="admonition-title">Chapter Summary</p>
<p><a class="reference internal" href="preprocessing.html#fairml-pre-processing"><span class="std std-ref"><strong>Pre-processing Algorithms</strong></span></a></p>
<p>Pre-processing algorithms make adjustments to the data to mitigate the unfairness of the downstream machine learning model. In this section, we will discuss the following approaches:</p>
<ul class="simple">
<li><p><a class="reference internal" href="preprocessing.html#relabeling"><span class="std std-ref">Relabelling</span></a></p></li>
<li><p><a class="reference internal" href="preprocessing.html#representation-learning"><span class="std std-ref">Representation learning</span></a></p></li>
</ul>
<p><a class="reference internal" href="constrainedlearning.html#fairml-constrained-learning"><span class="std std-ref"><strong>Constrained Learning Algorithms</strong></span></a></p>
<p>Constrained learning techniques directly incorporate a fairness constraint in the learning algorithm, typically by adjusting existing learning paradigms. In this section, we will discuss the following approaches:</p>
<ul class="simple">
<li><p><a class="reference internal" href="constrainedlearning.html#reweighing"><span class="std std-ref">Reweighing</span></a></p></li>
<li><p><a class="reference internal" href="constrainedlearning.html#regularization"><span class="std std-ref">Regularization</span></a></p></li>
<li><p><a class="reference internal" href="constrainedlearning.html#decoupled-classification"><span class="std std-ref">Decoupled Classification</span></a></p></li>
<li><p><a class="reference internal" href="constrainedlearning.html#adversarial-learning"><span class="std std-ref">Adversarial Learning</span></a></p></li>
</ul>
<p><a class="reference internal" href="postprocessing.html#fairml-post-processing"><span class="std std-ref"><strong>Post-processing Algorithms</strong></span></a></p>
<p>Post-processing approaches adjust an existing model, either through post-processing predictions or by adjusting the learned model parameters directly. In this section, we will discuss the following approaches:</p>
<ul class="simple">
<li><p><a class="reference internal" href="postprocessing.html#reject-option-classification"><span class="std std-ref">Reject-Option Classification</span></a></p></li>
<li><p><a class="reference internal" href="postprocessing.html#randomized-thresholds"><span class="std std-ref">Randomized Group-Specific Decision Thresholds</span></a></p></li>
<li><p><a class="reference internal" href="postprocessing.html#leaf-relabeling"><span class="std std-ref">Leaf Relabelling</span></a></p></li>
</ul>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<div class="docutils container" id="id3">
<dl class="footnote brackets">
<dt class="label" id="footcite-kamiran2012data"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination. <em>Knowledge and information systems</em>, 33(1):1–33, 2012.</p>
</dd>
<dt class="label" id="footcite-kamishima2012fairness"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-aware classifier with prejudice remover regularizer. In <em>Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part II 23</em>, 35–50. Springer, 2012.</p>
</dd>
</dl>
</div>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./fairness/fairml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../groupfairnessmetrics.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Group Fairness Metrics</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="preprocessing.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Pre-processing Algorithms</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Hilde Weerts<br/>
  
      &copy; Copyright 2024.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>