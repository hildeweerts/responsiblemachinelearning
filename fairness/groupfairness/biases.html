
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Biases as Sources of Unfairness &#8212; An Introduction to Responsible Machine Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Group Fairness Metrics" href="groupfairnessmetrics.html" />
    <link rel="prev" title="Measuring Fairness" href="introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">An Introduction to Responsible Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../index.html">
                    An Introduction to Responsible Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/introduction.html">
   Responsible Machine Learning
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../introduction/machinelearning/introduction.html">
   Machine Learning Preliminaries
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../introduction/machinelearning/modelevaluation.html">
     Model Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../introduction/machinelearning/modelselection.html">
     Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../introduction/machinelearning/costsensitivelearning.html">
     Cost-Sensitive Learning
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Fairness
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction.html">
   Algorithmic Fairness
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="introduction.html">
   Measuring Fairness
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Biases as Sources of Unfairness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="groupfairnessmetrics.html">
     Group Fairness Metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="normativeunderpinnings.html">
     Choosing the “Right” Fairness Metric
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../fairml/introduction.html">
   Fairness-Aware Machine Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../fairml/nofairnessthroughunawareness.html">
     No Fairness through Unawareness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../fairml/groupspecificthresholds.html">
     Group-Specific Decision Thresholds
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Explainability
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../explainability/introduction.html">
   Explainable Machine Learning
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../explainability/localposthoc/introduction.html">
   Local Post-Hoc Explanations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../explainability/localposthoc/lime.html">
     LIME
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../explainability/localposthoc/shap.html">
     SHAP
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../explainability/localposthoc/discussion.html">
     Feature importance
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../explainability/globalposthoc/introduction.html">
   Global Post-Hoc Explanations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../explainability/globalposthoc/pdp.html">
     Partial Dependence Plots
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Misc
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../misc/harms.html">
   List of Harms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../misc/glossary.html">
   Glossary
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/hildeweerts/responsiblemachinelearning"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/hildeweerts/responsiblemachinelearning/issues/new?title=Issue%20on%20page%20%2Ffairness/groupfairness/biases.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/fairness/groupfairness/biases.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#problem-understanding-abstraction-traps">
   Problem Understanding: Abstraction Traps
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-framing-trap">
     The Framing Trap
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-portability-trap">
     The Portability Trap
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-formalism-trap">
     The Formalism Trap
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-ripple-effect-trap">
     The Ripple Effect Trap
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-solutionism-trap">
     The Solutionism Trap
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-collection-and-processing">
   Data Collection and Processing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#historical-bias">
     Historical Bias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representation-bias">
     Representation Bias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#measurement-and-selection-bias">
     Measurement and Selection Bias
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modeling">
   Modeling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aggregation-bias">
     Aggregation bias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#omitted-variable-bias">
     Omitted variable bias
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation">
   Evaluation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deployment">
   Deployment
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#usage">
     Usage
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretation">
     Interpretation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interaction-reinforcing-feedback-loops">
     Interaction: Reinforcing Feedback Loops
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Biases as Sources of Unfairness</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#problem-understanding-abstraction-traps">
   Problem Understanding: Abstraction Traps
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-framing-trap">
     The Framing Trap
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-portability-trap">
     The Portability Trap
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-formalism-trap">
     The Formalism Trap
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-ripple-effect-trap">
     The Ripple Effect Trap
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-solutionism-trap">
     The Solutionism Trap
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-collection-and-processing">
   Data Collection and Processing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#historical-bias">
     Historical Bias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representation-bias">
     Representation Bias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#measurement-and-selection-bias">
     Measurement and Selection Bias
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modeling">
   Modeling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aggregation-bias">
     Aggregation bias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#omitted-variable-bias">
     Omitted variable bias
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation">
   Evaluation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deployment">
   Deployment
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#usage">
     Usage
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretation">
     Interpretation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interaction-reinforcing-feedback-loops">
     Interaction: Reinforcing Feedback Loops
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="biases-as-sources-of-unfairness">
<span id="biases-problem"></span><h1>Biases as Sources of Unfairness<a class="headerlink" href="#biases-as-sources-of-unfairness" title="Permalink to this headline">#</a></h1>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This section is still under construction.</p>
</div>
<!-- TODO: add parts about noise/variance by Chen et al.-->
<p>Algorithmic systems are an accumulation of design choices that embed the developers’ explicit and implicit value judgements into the system <a class="footnote-reference brackets" href="#footcite-wieringa2020" id="id1">1</a>. Consequently, biases can seep into the system in many different places of the development process. In this section, we will explore biases as sources of unfairness in different parts of the machine learning development process.</p>
<p>But first, let us define more precisely what we mean by bias. Generally speaking, <span class="xref std std-term">bias</span> is a systematic and disproportionate tendency towards something. The word bias can be used to refer to many different things, ranging from social biases related to prejudice, to statistical biases that are of a technical nature.</p>
<ul class="simple">
<li><p><strong>Social and systemic bias.</strong> In everyday language, bias often considers prejudice against a person or a group, which we will refer to as <span class="xref std std-term">social bias</span>. Social bias is a form of <span class="xref std std-term">cognitive bias</span>: a systematic error in rational thinking that can affect judgment and decision-making. Most <span class="xref std std-term">cognitive bias</span>es are a result of the limitations of information-processing capabilities of the human brain. From an evolutionary perspective, these biases are useful because they allow people to make quick decisions in critical scenarios. As a hunter-gatherer, you would probably rather be safe than sorry when encountering an unknown group of other humans. However, these shortcuts often come at the cost of the quality of decisions. In particular, stereotypes formed by social bias can be overgeneralized and inaccurate, especially on an individual level. When people act on social biases, they can result in discriminatory practices. It is important to realize that everybody has some degree of conscious or subconscious social bias. Awareness of <span class="xref std std-term">cognitive bias</span>es, including social bias, can help to signal and mitigate their effects. In particular, a diverse team and critical self-reflection can help to signal social biases and avoid acting on them.</p></li>
<li><p><strong>Statistical bias.</strong> In statistics, bias refers to a systematic error in the estimation of parameters or variables. <span class="xref std std-term">statistical bias</span> can be the result of data collection practices that compromise the accuracy of the estimate, such as a particular sampling procedure. This form of statistical bias can be rooted in <span class="xref std std-term">cognitive bias</span>es of the researcher or data subjects. Statistical bias can also refer to systematic errors caused by assumptions of the estimator. In the context of machine learning algorithms, this type of bias is often discussed in relation to the bias-variance trade-off. For example, some machine learning algorithms can only learn linear relationships between features, whereas the true underlying data distribution exhibits more complex relationships, resulting in an <a class="reference internal" href="../../misc/glossary.html#term-underfitting"><span class="xref std std-term">underfitting</span></a> model.</p></li>
</ul>
<p>Although all these different types of bias can result in fairness related harms, most issues arise at the intersection of social bias and statistical bias. In the remainder of this section, we will dive more deeply into different types of biases at each stage of the development process. Note that this list is not exhaustive. Moreover, as we will see, biases are hard to precisely dissect and often overlap. In practice, it is usually difficult if not impossible to know exactly which biases are at play. Luckily, a thoughtful development process that leaves room for self-reflection goes a long way in mitigating harms.</p>
<div class="section" id="problem-understanding-abstraction-traps">
<h2>Problem Understanding: Abstraction Traps<a class="headerlink" href="#problem-understanding-abstraction-traps" title="Permalink to this headline">#</a></h2>
<p>Translating a real-world problem into a machine learning task can be difficult. By definition, a model is a simplification of reality. A data scientist’s task is to decide which elements of the real world need to be included in the model and which elements will be left out of scope. To this end, some amount of abstraction is required. This involves removing details to focus attention on general patterns. The impact of your system, both positive and negative, highly depends on how you define the machine learning task. By abstracting away the context surrounding a model and its inputs and outputs, you may accidentally abstract away some of the consequences as well.</p>
<p>A mismatch between the machine learning task and the real-world context is referred to as an <span class="xref std std-term">abstraction trap</span> <a class="footnote-reference brackets" href="#footcite-selbst2019" id="id2">2</a>. Abstraction traps can amplify harmful consequences of your system, particularly those related to fairness.</p>
<div class="section" id="the-framing-trap">
<h3>The Framing Trap<a class="headerlink" href="#the-framing-trap" title="Permalink to this headline">#</a></h3>
<p>Machine learning models hardly ever operate in isolation. A decision-making process may incorporate (multiple) other machine learning models or human decision-makers. The <span class="xref std std-term">framing trap</span> considers the failure to model the relevant aspects of the larger system your machine learning model is a part of.</p>
<p>For example, consider a scenario in which judges need to decide whether a defendant is detained. To assist them in their decision making process, they may be provided with a machine learning model that predicts the risk of recidivism; i.e., the risk that the defendant will re-offend. Notably, the final decision of the judge determines the real-world consequences, not the model’s prediction. Hence, if fairness is a requirement, it is not sufficient to consider the output of the model; you also need to consider how the predictions are used by the judges.</p>
<p>In many real-world systems, several machine learning models are deployed at the same time at different points in the decision-making process. Unfortunately, a system of components that seem fair in isolation do not automatically imply a fair system, i.e. \textit{Fair + Fair <span class="math notranslate nohighlight">\(\neq\)</span> Fair} <a class="footnote-reference brackets" href="#footcite-dwork2020" id="id3">3</a>.</p>
<p>To avoid the framing trap, we need to ensure that the way we frame our problem and evaluate our solution includes all relevant components and actors of the sociotechnical system.</p>
</div>
<div class="section" id="the-portability-trap">
<h3>The Portability Trap<a class="headerlink" href="#the-portability-trap" title="Permalink to this headline">#</a></h3>
<p>A system that is carefully designed for a particular context cannot always be directly applied in a different context. Taking an existing solution and applying it in a different situation without taking into account the differences between the two contexts is known as the <span class="xref std std-term">portability trap</span>. A shift in domain, geographical location, time, or even the nature of the decision-making process all impact the suitability of a system. For example, a voice recognition system optimized for speakers with an Australian accent may fail horribly when deployed in the United States. Similarly, the expectations of a good manager have changed considerably in the past few decades, including a stronger need for soft skills. A model trained on annual reviews in the 1960’s will likely not be suitable to make predictions for current managers. The <span class="xref std std-term">portability trap</span> goes beyond performance issues due to differences in the data distribution. It also considers differences in social norms and actors. For example, a chat bot optimized to formulate snarky replies may be considered funny on a gaming platform, but inappropriate or even offensive in a more formal context, such as a website for loan applications. To avoid falling into the <span class="xref std std-term">portability trap</span>, we need to consider whether our problem understanding adequately models the social and technical requirements of the actual deployment context.</p>
</div>
<div class="section" id="the-formalism-trap">
<h3>The Formalism Trap<a class="headerlink" href="#the-formalism-trap" title="Permalink to this headline">#</a></h3>
<p>In order to use machine learning, you need to formulate your problem in a way that a mathematical algorithm can understand. This is not a straightforward task: there are usually many different ways to measure something. Some may be more appropriate than others. You fall into the <span class="xref std std-term">formalism trap</span> when your formalization does not adequately take into account the context in which your model will be used. For example, machine learning problem formulations often simplify the decision space to a very limited set of actions <a class="footnote-reference brackets" href="#footcite-mitchell2018" id="id4">4</a>. In lending, the decision space of the machine learning model may consist of two options: reject or accept. In reality, there may be many more actions available, such as recommending a different type of loan.</p>
<p>The <span class="xref std std-term">formalism trap</span> is closely related to the statistical concept of <span class="xref std std-term">construct validity</span>: how well does the formalization measure the construct of interest? Business objectives often involve constructs such as “employee quality” or “creditworthiness” that cannot be measured directly <a class="footnote-reference brackets" href="#footcite-jacobs2019" id="id5">5</a>. In such cases, data scientists may use a <span class="xref std std-term">proxy variable</span> instead. Every variable in a data set is the result of a decision on how a particular construct can be measured into a computer readable scale. For example, Netflix has chosen to measure a viewers’ quality judgments with likes, rather than the more commonly used 1 - 5 star rating <a class="footnote-reference brackets" href="#footcite-dobbe2018" id="id6">6</a>.</p>
<p>Not all variables measure the intended construct equally well. <span class="xref std std-term">construct validity bias</span> is a statistical bias that occurs when a variable does not accurately measure the construct it is supposed to measure. For example, income measures the construct socioeconomic status to some degree, but does not capture other factors such as wealth and education <a class="footnote-reference brackets" href="#footcite-jacobs2019" id="id7">5</a>.</p>
<p>A mismatch between the choice of target variable and the actual construct of interest can be detrimental towards fairness goals. In particular, fairness concerns can arise when the measurement error introduced by the choice of formalization differs across groups. For example, you may be interested in predicting crime, but only have access to a subset of all criminal activity: arrest records. In societies where arrest records are the result of racially biased policing practices, the measurement error will differ across racial groups. Similarly, Obermeyer <em>et al.</em><a class="footnote-reference brackets" href="#footcite-obermeyer2019" id="id8">7</a> found that due to unequal access to healthcare, historically less money has been spent on caring for African-American patients compared to Caucasian patients. Consequently, a system that used healthcare costs as a proxy for true healthcare needs systematically underestimated the needs of African-American patients.</p>
<p>Issues of <span class="xref std std-term">construct validity</span> are especially complex for social constructs such as race and gender. Almost paradoxically, measuring <a class="reference internal" href="../../misc/glossary.html#term-sensitive-characteristic"><span class="xref std std-term">sensitive characteristic</span></a> can introduce bias. In industry and academia, it is common to “infer” these characteristics from observed data, such as facial analysis <a class="footnote-reference brackets" href="#footcite-jacobs2019" id="id9">5</a>. This can be problematic because such approaches often fail to acknowledge that social constructs are inherently contextual, may change over time, and are multidimensional. For example, when talking about race, one may be referring to somebody’s racial identity (i.e., self-identified race), their observed race (i.e., the race others believe them to be), their phenotype (i.e., racial appearance), or even their reflected race (i.e., the race they believe others assume them to be). Which dimension you measure will influence the conclusions you can draw (see Hanna <em>et al.</em><a class="footnote-reference brackets" href="#footcite-hanna2020" id="id10">8</a> for a more detailed account).</p>
<p>To avoid falling into the <span class="xref std std-term">formalism trap</span>, data scientists should take into account whether the problem formulation handles understandings of (social) constructs in a way that matches the intended deployment context. To mitigate <span class="xref std std-term">construct validity bias</span>, ideally multiple measures are collected, especially for complex constructs. As we will see, many of the biases that can occur in later steps of the development process can be traced back to the problem of <span class="xref std std-term">construct validity</span>.</p>
</div>
<div class="section" id="the-ripple-effect-trap">
<h3>The Ripple Effect Trap<a class="headerlink" href="#the-ripple-effect-trap" title="Permalink to this headline">#</a></h3>
<p>Introducing a machine learning model in a social context may affect the behavior of other actors in the system and, as a result, the context itself. This is known as the <span class="xref std std-term">ripple effect trap</span>. There are several ways in which a social context may change due to the introduction of a new technology. First, the introduction of a new technology might be used to argue for or reinforce power, which can change an organization’s dynamics. For example, management may purchase software for monitoring workers, reinforcing the power relationship between management and subordinates. Second, the introduction of a prediction systems may cause reactivity behavior. For example, people might attempt to game an automated loan approval system by dishonestly filling out their data in the hope of a more favorable outcome. Third, a system that was developed for a particular use case, may be used in unintended, perhaps even adversarial, ways. To avoid falling into the <span class="xref std std-term">ripple effect trap</span>, it is important to consider whether the envisioned system changes the context in a predictable way.</p>
</div>
<div class="section" id="the-solutionism-trap">
<h3>The Solutionism Trap<a class="headerlink" href="#the-solutionism-trap" title="Permalink to this headline">#</a></h3>
<p>The possible benefits that machine learning solutions can bring can be very exciting. Unfortunately, machine learning is not the answer to everything (\textit{what?!}). The belief that every problem has a technological solution is referred to as <span class="xref std std-term">solutionism</span>. We fall into the <span class="xref std std-term">solutionism trap</span> when we fail to recognize the machine learning is not the right tool for the problem at hand.</p>
<p>The <span class="xref std std-term">solutionism trap</span> is closely related to the <span class="xref std std-term">optimism bias</span>. This is a <span class="xref std std-term">cognitive bias</span> that causes people to overestimate the likelihood of positive events and underestimate the likelihood of negative events. In the context of algorithmic systems, <span class="xref std std-term">optimism bias</span> occurs when policy makers or developers are overly optimistic about a system’s benefits, while underestimating its limitations and weaknesses. In particular, people might overestimate the objectiveness of data and algorithmic systems. If this happens, the system’s goals, development, and outcomes might not be sufficiently scrutinized, which can result in systematic harms.</p>
<p>There are several reasons why machine learning may not be the right tool to solve a problem. In some scenarios, it may not be possible to adequately model the context using automated data collection. For example, consider eligibility for social welfare benefits in the Netherlands. Although the criteria for eligibility are set in the law, some variables, e.g. living situation, are difficult to measure quantitatively. Moreover, the Dutch legal system contains the possibility to deviate from the criteria due to compelling personal circumstances. It is impossible to anticipate all context-dependent situations in advance. As a result, machine learning may not be the best tool for this job. In other scenarios, machine learning may be inappropriate because it lacks human connection. For example, consider a person who is hospitalized. In theory, it may be possible to develop a robot nurse who is perfectly capable of performing tasks such as inserting an IV or washing the patient. However, the patient may also value the genuine interest and concern of a nurse  – in other words, a human connection, something a machine learning model cannot (or even should not) provide. Furthermore, there may be cases where machine learning is simply overkill. For example, you may wonder whether spending several months on optimizing a deep learning computer vision system to predict the dimensions of items in your online shop is a better approach than simply asking the person who puts the item on the website to fill out the dimensions.</p>
<p>To avoid falling into the <span class="xref std std-term">solutionism trap</span>, it is useful to consider machine learning as a means to an end. In other words, rather than asking “can we use machine learning”, ask, “how can we solve this problem?” and then consider machine learning as one of the options.</p>
</div>
</div>
<div class="section" id="data-collection-and-processing">
<h2>Data Collection and Processing<a class="headerlink" href="#data-collection-and-processing" title="Permalink to this headline">#</a></h2>
<p>It may not come as a surprise that many fairness issues arise through biases in data collection and analysis. We will discuss three forms of bias that may be present in the data: <span class="xref std std-term">historical bias</span>, <span class="xref std std-term">representation bias</span>, <span class="xref std std-term">measurement bias</span>.</p>
<div class="section" id="historical-bias">
<h3>Historical Bias<a class="headerlink" href="#historical-bias" title="Permalink to this headline">#</a></h3>
<p>Social biases can be encoded in data. If not accounted for, a machine learning model will reproduce these biases, resulting in unfair outcomes. Generally speaking, <span class="xref std std-term">historical bias</span> comes in two flavors.</p>
<p>Firstly, <span class="xref std std-term">historical bias</span> can arise due to social biases in human decision-making. This type of bias is particularly prevalent when target labels are based on human judgement. For example, if in the past more men have been hired than women, a model trained on historical decisions will likely reproduce this association. As discussed in the previous section, simply removing the <span class="xref std std-term">sensitive feature</span> is not likely to remove this type of bias due to associations between features. Note that this type of <span class="xref std std-term">historical bias</span> is a form of <span class="xref std std-term">construct validity bias</span>: the historical hiring decisions are a biased proxy for actual suitability of the applicant. Similarly, inaccurate stereotypes can be embedded in texts, images, and annotations produced by people, resulting in systems that reinforce these stereotypes.</p>
<p>A second type of <span class="xref std std-term">historical bias</span>  occurs when the data is a good representation of reality, but reality is biased. In our hiring example, the observed bias could be caused by actual differences in suitability, but these differences are in turn caused by structural inequalities in society. For example, people from lower socioeconomic backgrounds may have had fewer opportunities to get good education, making them less suitable for jobs where such education is required for job performance. Similarly, some stereotypes are accurate at an aggregate level (even if they can be very inaccurate at an individual level!). For example, in many societies female nurses still greatly outnumber male nurses.</p>
<p>Depending on your worldview, you might have a different definition of what is fair in each of these scenarios. In practice, it is usually impossible to distinguish between these two types of <span class="xref std std-term">historical bias</span> from observed data alone. Moreover, they can also occur simultaneously. Consequently, understanding <span class="xref std std-term">historical bias</span> and identifying a mitigation approach that is in line with your own moral values requires a deep understanding of the social context.</p>
</div>
<div class="section" id="representation-bias">
<h3>Representation Bias<a class="headerlink" href="#representation-bias" title="Permalink to this headline">#</a></h3>
<p><span class="xref std std-term">representation bias</span> occurs when some groups are underrepresented in the data <a class="footnote-reference brackets" href="#footcite-suresh2020" id="id11">9</a>. A machine learning model might not generalize well for underrepresented groups, causing <span class="xref std std-term">quality-of-service harm</span>. <span class="xref std std-term">Representation bias</span> is especially risky when the data distribution of minority groups differs substantially from the majority group (see also <span class="xref std std-term">aggregation bias</span>). A well-known example of <span class="xref std std-term">representation bias</span> was uncovered by Buolamwini and Gebru<a class="footnote-reference brackets" href="#footcite-buolamwini2018" id="id12">10</a>. They found that the data sets that were used to train commercial facial recognition systems contained predominantly images of white men. Consequently, the models did not generalize well to people with dark skin, especially women.</p>
<p><span class="xref std std-term">Representation bias</span> is closely related to <span class="xref std std-term">selection bias</span>, a statistical bias that occurs when the data collection or selection results in a non-random sample of the population. If not taken into account, conclusions regarding the studied effect may be wrong. For example, young healthy people may be more likely to volunteer for a vaccine trial than less healthy older people. As a result, conclusions about the side effects may not be representative for the whole population. Notably, <span class="xref std std-term">representation bias</span> can occur even when a sample is truly random, as there may not be sufficient information available for minority groups. Moreover, <span class="xref std std-term">representation bias</span> can be an issue in both training and testing data.</p>
<p>An underlying cause of <span class="xref std std-term">representation bias</span> are blind spots of the collectors. For example, a data science team that consists solely of women is less likely to notice that men are not well represented in the data than a more diverse team. Additionally, some data is easier to get than others. For example, collecting data on the interests of young adults, who often spend hours each day scrolling through their social media feeds, is much easier compared to the interests elderly people who are generally not as active online.</p>
<p><span class="xref std std-term">representation bias</span> is relatively easy to solve by getting more data. Additionally, data scientists can leverage techniques that were designed to deal with sampling errors, such as weighting instances. However, these approaches first require you to identify cases in which <span class="xref std std-term">representation bias</span>may occur. Therefore, a more durable solution is to invest in a diverse and inclusive development approach, which will help to avoid leaving groups out of the picture in the first place.</p>
</div>
<div class="section" id="measurement-and-selection-bias">
<h3>Measurement and Selection Bias<a class="headerlink" href="#measurement-and-selection-bias" title="Permalink to this headline">#</a></h3>
<p><span class="xref std std-term">measurement bias</span> is a statistical bias that occurs when data contains systematic errors, due to data collection practices. <span class="xref std std-term">measurement bias</span> can be a cause of <span class="xref std std-term">construct validity bias</span>. If systematic errors are correlated to <span class="xref std std-term">sensitive characteristics</span>, <span class="xref std std-term">measurement bias</span> can become a source of unfairness.</p>
<p><span class="xref std std-term">measurement bias</span> can occur when the method of observation results in systematic errors. First of all, the measurement process may be different across groups <a class="footnote-reference brackets" href="#footcite-suresh2020" id="id13">9</a>, due to a combination of social bias and <span class="xref std std-term">confirmation bias</span>. <span class="xref std std-term">confirmation bias</span> is a <span class="xref std std-term">cognitive bias</span> that refers to people’s tendency to look for evidence of existing beliefs and disregard evidence that goes against it. In the context of data analysis, <span class="xref std std-term">confirmationbias</span> can lead to cherry picking data that support a conclusion. As the famous quote by Ronald Coase says: <em>“if you torture the data long enough, it will confess to anything”</em> For example, a fraud analyst might overly scrutinize some groups over others. Higher rates of testing will result in more positives, confirming the analysts biased beliefs and skewing the observed base rates. If not accounted for, these skewed numbers will be reproduced by the machine learning model.</p>
<p>Another example of <span class="xref std std-term">measurement bias</span> occurs when different observers interpret reality differently. In medical studies, for example, clinicians might arbitrarily round blood pressure readings up or down to the nearest whole number, depending on what they expect to see. In a machine learning context, this type of bias can occur when annotations reflect social biases, such as stereotypes, of the annotators or decision-makers.</p>
<p><span class="xref std std-term">Measurement bias</span> can also occur at the side of the data subject. Data subjects might behave differently because they are being observed, especially when data is collected from memory or through self-reporting. Survey responses may be incomplete or inconsistent because participants try to present themselves in a way that is socially desirable. For example, consider self-reported height, scraped from a dating website. In many cultures, tallness is seen as an attractive trait in men. Consequently, men may have exaggerated their height to appear more attractive, resulting in <span class="xref std std-term">measurement bias</span>. Note that this is another example of how <span class="xref std std-term">measurement bias</span> can be a threat to <span class="xref std std-term">construct validity</span>.</p>
<p><span class="xref std std-term">measurement bias</span> can be mitigated by high-quality data collection procedures. Note that it is not possible to identify cases of <span class="xref std std-term">measurement bias</span> from observational data alone. For example, we cannot know the true underlying fraud rate if we only take into account data produced by a biased fraud detection approach. This highlights the importance of documenting the data collection procedure. % This does not only hold for data that was collected within your organization. Take into account possible <span class="xref std std-term">measurement bias</span> when considering external data sources as well.</p>
</div>
</div>
<div class="section" id="modeling">
<h2>Modeling<a class="headerlink" href="#modeling" title="Permalink to this headline">#</a></h2>
<p>Building a machine learning model includes many different choices, ranging from the class of machine learning algorithms that is considered to their hyperparameter settings. Different models may have different consequences related to fairness, depending on the task at hand.</p>
<div class="section" id="aggregation-bias">
<h3>Aggregation bias<a class="headerlink" href="#aggregation-bias" title="Permalink to this headline">#</a></h3>
<p><span class="xref std std-term">aggregationbias</span> occurs when a single model is used for groups that have distinct data distributions <a class="footnote-reference brackets" href="#footcite-suresh2020" id="id14">9</a>. If not accounted for, it may lead to a model that does not work well for any of the subgroups. For example, it is known that the relationship between hemoglobin levels (HbA1c) and blood glucose levels differs greatly across genders and ethnicities <a class="footnote-reference brackets" href="#footcite-suresh2020" id="id15">9</a>. If these differences are not taken into account, a model that only uses a single interpretation of HbA1c will likely not work well for any of these groups. In combination with <span class="xref std std-term">representationbias</span>, it can lead to a model that only works well for the majority population.</p>
<p><span class="xref std std-term">aggregation bias</span> is related to the problem of <a class="reference internal" href="../../misc/glossary.html#term-underfitting"><span class="xref std std-term">underfitting</span></a>. Machine learning can be seen as a compression problem that produces a mapping between input features and output variable. Some information is inherently lost because of the chosen mapping <a class="footnote-reference brackets" href="#footcite-dobbe2018" id="id16">6</a>. In particular, some model classes may not be able to adequately capture the different data distributions. Such an oversimplified model may come at the cost of predictive performance for minority groups, resulting in <span class="xref std std-term">quality-of-service harm</span>.</p>
</div>
<div class="section" id="omitted-variable-bias">
<h3>Omitted variable bias<a class="headerlink" href="#omitted-variable-bias" title="Permalink to this headline">#</a></h3>
<p><span class="xref std std-term">Omitted variable bias</span> is a statistical bias which occurs when one or more relevant features are left out of a linear regression model. Consequently, the model attributes the effects of the missing feature(s) to the included features, obscuring their true effects. <span class="xref std std-term">Omitted variable bias</span> was originally introduced in the context of statistical models that are used for causal inference. For example, consider a regression-based test for discrimination in loan applications. Imagine that loan officers consider payment history in their decision and that payment history correlates with race. If payment history is not recorded in the data, the results of the regression will attribute the effect of payment history to race, suggesting <span class="xref std std-term">direct discrimination</span> and potentially <span class="xref std std-term">procedural harm</span> when there is not <a class="footnote-reference brackets" href="#footcite-jung2019omitted" id="id17">11</a>. This bias can also be relevant for prediction tasks. In particular, excluding <span class="xref std std-term">sensitive feature</span>s from the data may obscure a model’s indirect dependence on this feature, attributing their effects to other related features. This makes it more difficult to detect and account for existing <span class="xref std std-term">historical bias</span>.</p>
</div>
</div>
<div class="section" id="evaluation">
<h2>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">#</a></h2>
<p>During the evaluation stage, the final model is scrutinized in more detail. <span class="xref std std-term">evaluation bias</span> refers to the use of performance metrics and procedures that are not appropriate for the way in which the model will be used <a class="footnote-reference brackets" href="#footcite-suresh2020" id="id18">9</a>. Mitchell <em>et al.</em><a class="footnote-reference brackets" href="#footcite-mitchell2018" id="id19">4</a> identify several underlying assumptions of performance metrics. First, these metrics assume that individual decisions are independent of each other. Note how this assumption is grounded in utilitarianism, in which overall utility is expressed as the sum of individual utilities. In practice, however, the impact of a decision may not be independent across instances. For example, denying one family member a loan may impact another family member’s ability to repay their own loan. Additionally, it is typically assumed that decisions are symmetrical, i.e. the impact of the outcome is equal across instances. Again, this often does not hold in practice. For example, a rejection of a job application can have a very different impact depending on whether that person is currently employed or unemployed.</p>
</div>
<div class="section" id="deployment">
<h2>Deployment<a class="headerlink" href="#deployment" title="Permalink to this headline">#</a></h2>
<p>Once the system is deployed, it may be used, interpreted, or interacted with inappropriately, resulting in unfair outcomes <a class="footnote-reference brackets" href="#footcite-friedman1996" id="id20">12</a>. The underlying cause of these outcomes is a mismatch between the system’s design and the context in which it will be applied. Indeed, biases in deployment can often be attributed to abstraction traps.</p>
<div class="section" id="usage">
<h3>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">#</a></h3>
<p>The system may be used in a context for which it was not (properly) designed, in which case we fall into the <span class="xref std std-term">portability trap</span>. For example, a toxic language detection model trained on tweets may not be suitable for a platform such as TikTok, where the average user is much younger and may use different language (tone, words, etc.) than an average Twitter user. Note that this type of bias can also accrue over time due to changing populations and behaviors, in which case it can be seen as a form of concept drift.</p>
</div>
<div class="section" id="interpretation">
<h3>Interpretation<a class="headerlink" href="#interpretation" title="Permalink to this headline">#</a></h3>
<p>Interaction of stakeholders with the system can be a source of unfairness. A decision-maker may interpret the model’s output differently for different groups, due to social bias and <span class="xref std std-term">confirmation bias</span>. For example, a judge may weigh a high risk score more heavily for a black defendant compared to a white defendant, due to (unconscious) social bias. This bias, which can be attributed to falling into the framing trap, can be mitigated by taking into account stakeholder interactions during the system’s design and evaluation.</p>
</div>
<div class="section" id="interaction-reinforcing-feedback-loops">
<h3>Interaction: Reinforcing Feedback Loops<a class="headerlink" href="#interaction-reinforcing-feedback-loops" title="Permalink to this headline">#</a></h3>
<p>In systems that learn from user interactions, users can introduce social bias. For example, consider a chat bot that learns dynamically. Without safeguards against toxicity, users might teach it to use obscene or otherwise offensive language, resulting in <span class="xref std std-term">denigration harm</span>. This type of bias can be avoided by putting checks in place to identify malicious intent towards the system.</p>
<p>Feedback mechanisms that amplify an effect are called <span class="xref std std-term">reinforcing feedback loop</span>s. In the context of fairness, it refers to the amplification of existing (historical) biases when new data is collected based on the output of a biased model.</p>
<div class="admonition-example-a-reinforcing-feedback-loop-in-predictive-policing admonition">
<p class="admonition-title"><em>Example:</em> A Reinforcing Feedback Loop in Predictive Policing</p>
<p>Lets imagine there is a police station that is responsible for two neighborhoods, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. Now lets imagine a predictive policing system that allocates police officers to the neighborhoods based on the predicted crime rate in each neighborhood. In this example, the true crime rates of the neighborhoods are equal. However, due to the randomness, we have collected slightly more crime data in neighborhood <span class="math notranslate nohighlight">\(A\)</span> than than in neighborhood <span class="math notranslate nohighlight">\(B\)</span> at the time the prediction model is trained. Consequently, the model predicts more crime in neighborhood <span class="math notranslate nohighlight">\(A\)</span> than in neighborhood <span class="math notranslate nohighlight">\(B\)</span>. Based on this prediction, we send more police officers to neighborhood <span class="math notranslate nohighlight">\(A\)</span>. Consequently, more crime will be detected in neighborhood <span class="math notranslate nohighlight">\(A\)</span> – even though the true crime rates are the same. If we retrain our model on the newly collected crime data, even more police officers will be allocated to neighborhood <span class="math notranslate nohighlight">\(A\)</span> and even more crime is detected. And so the feedback loop continues…</p>
</div>
<p>A consequence of these feedback loops is that people can form erroneous beliefs based on the data. For example, after the introduction of the predictive policing system in our example, police officers may believe that neighborhood <span class="math notranslate nohighlight">\(A\)</span> truly has a bigger crime problem than neighborhood <span class="math notranslate nohighlight">\(B\)</span>. A failure to anticipate on feedback loops can be particularly risky for automated decision-making systems, in which bias can propagate quickly over time.</p>
<!-- A specific instance of feedback loops that recommender systems may suffer from is {term}`popularity bias`. If people tend to click on highly ranked items more often, this can lead the algorithm to rank popular items even higher and disregard less popular items that may be just as valuable to the user. 
One way to investigate feedback loops is through simulation. Developing an accurate simulation of a sociotechnical system is difficult and requires a lot of domain expertise. Alternatively, we may borrow approaches from the field of system dynamics {footcite:p}`martin2020extending` and causal modeling. -->
<div class="docutils container" id="id21">
<dl class="footnote brackets">
<dt class="label" id="footcite-wieringa2020"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Maranke Wieringa. What to account for when accounting for algorithms. In <em>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>. ACM, January 2020. URL: <a class="reference external" href="https://doi.org/10.1145/3351095.3372833">https://doi.org/10.1145/3351095.3372833</a>, <a class="reference external" href="https://doi.org/10.1145/3351095.3372833">doi:10.1145/3351095.3372833</a>.</p>
</dd>
<dt class="label" id="footcite-selbst2019"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Andrew D. Selbst, Danah Boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, and Janet Vertesi. Fairness and abstraction in sociotechnical systems. <em>FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency</em>, pages 59–68, 2019. <a class="reference external" href="https://doi.org/10.1145/3287560.3287598">doi:10.1145/3287560.3287598</a>.</p>
</dd>
<dt class="label" id="footcite-dwork2020"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Cynthia Dwork, Christina Ilvento, and Meena Jagadeesan. Individual fairness in pipelines. <em>arXiv preprint arXiv:2004.05167</em>, 2020.</p>
</dd>
<dt class="label" id="footcite-mitchell2018"><span class="brackets">4</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id19">2</a>)</span></dt>
<dd><p>Shira Mitchell, Eric Potash, Solon Barocas, Alexander D’Amour, and Kristian Lum. Prediction-based decisions and fairness: a catalogue of choices, assumptions, and definitions. 2018. <a class="reference external" href="https://arxiv.org/abs/1811.07867">arXiv:1811.07867</a>.</p>
</dd>
<dt class="label" id="footcite-jacobs2019"><span class="brackets">5</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id7">2</a>,<a href="#id9">3</a>)</span></dt>
<dd><p>Abigail Z. Jacobs and Hanna Wallach. Measurement and fairness. In <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, FAccT ‘21, 375–385. New York, NY, USA, 2021. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3442188.3445901">https://doi.org/10.1145/3442188.3445901</a>, <a class="reference external" href="https://doi.org/10.1145/3442188.3445901">doi:10.1145/3442188.3445901</a>.</p>
</dd>
<dt class="label" id="footcite-dobbe2018"><span class="brackets">6</span><span class="fn-backref">(<a href="#id6">1</a>,<a href="#id16">2</a>)</span></dt>
<dd><p>Roel Dobbe, Sarah Dean, Thomas Gilbert, and Nitin Kohli. A broader view on bias in automated decision-making: reflecting on epistemology and dynamics. <em>arXiv preprint arXiv:1807.00553</em>, 2018.</p>
</dd>
<dt class="label" id="footcite-obermeyer2019"><span class="brackets"><a class="fn-backref" href="#id8">7</a></span></dt>
<dd><p>Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. Dissecting racial bias in an algorithm used to manage the health of populations. <em>Science</em>, 366(6464):447–453, 2019. URL: <a class="reference external" href="https://science.sciencemag.org/content/366/6464/447">https://science.sciencemag.org/content/366/6464/447</a>, <a class="reference external" href="https://arxiv.org/abs/https://science.sciencemag.org/content/366/6464/447.full.pdf">arXiv:https://science.sciencemag.org/content/366/6464/447.full.pdf</a>, <a class="reference external" href="https://doi.org/10.1126/science.aax2342">doi:10.1126/science.aax2342</a>.</p>
</dd>
<dt class="label" id="footcite-hanna2020"><span class="brackets"><a class="fn-backref" href="#id10">8</a></span></dt>
<dd><p>Alex Hanna, Emily Denton, Andrew Smart, and Jamila Smith-Loud. Towards a critical race methodology in algorithmic fairness. In <em>FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>, 501–512. dec 2020. URL: <a class="reference external" href="https://arxiv.org/abs/1912.03593">https://arxiv.org/abs/1912.03593</a>, <a class="reference external" href="https://arxiv.org/abs/1912.03593">arXiv:1912.03593</a>, <a class="reference external" href="https://doi.org/10.1145/3351095.3372826">doi:10.1145/3351095.3372826</a>.</p>
</dd>
<dt class="label" id="footcite-suresh2020"><span class="brackets">9</span><span class="fn-backref">(<a href="#id11">1</a>,<a href="#id13">2</a>,<a href="#id14">3</a>,<a href="#id15">4</a>,<a href="#id18">5</a>)</span></dt>
<dd><p>Harini Suresh and John V. Guttag. A framework for understanding unintended consequences of machine learning. 2020. <a class="reference external" href="https://arxiv.org/abs/1901.10002">arXiv:1901.10002</a>.</p>
</dd>
<dt class="label" id="footcite-buolamwini2018"><span class="brackets"><a class="fn-backref" href="#id12">10</a></span></dt>
<dd><p>Joy Buolamwini and Timnit Gebru. Gender shades: intersectional accuracy disparities in commercial gender classification. In <em>Conference on fairness, accountability and transparency</em>, 77–91. 2018.</p>
</dd>
<dt class="label" id="footcite-jung2019omitted"><span class="brackets"><a class="fn-backref" href="#id17">11</a></span></dt>
<dd><p>Jongbin Jung, Sam Corbett-Davies, Ravi Shroff, and Sharad Goel. Omitted and included variable bias in tests for disparate impact. 2019. <a class="reference external" href="https://arxiv.org/abs/1809.05651">arXiv:1809.05651</a>.</p>
</dd>
<dt class="label" id="footcite-friedman1996"><span class="brackets"><a class="fn-backref" href="#id20">12</a></span></dt>
<dd><p>Batya Friedman and Helen Nissenbaum. Bias in computer systems. <em>ACM Transactions on Information Systems</em>, 14(3):330–347, July 1996. URL: <a class="reference external" href="https://doi.org/10.1145/230538.230561">https://doi.org/10.1145/230538.230561</a>, <a class="reference external" href="https://doi.org/10.1145/230538.230561">doi:10.1145/230538.230561</a>.</p>
</dd>
</dl>
</div>
<hr class="footnotes docutils" />
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./fairness/groupfairness"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="introduction.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Measuring Fairness</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="groupfairnessmetrics.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Group Fairness Metrics</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Hilde Weerts<br/>
  
      &copy; Copyright 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>