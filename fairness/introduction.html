
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Algorithmic Fairness &#8212; An Introduction to Responsible Machine Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Group Fairness Metrics" href="groupfairnessmetrics.html" />
    <link rel="prev" title="Cost-Sensitive Learning" href="../introduction/machinelearning/costsensitivelearning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">An Introduction to Responsible Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    An Introduction to Responsible Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction/introduction.html">
   Responsible Machine Learning
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../introduction/machinelearning/introduction.html">
   Machine Learning Preliminaries
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../introduction/machinelearning/modelevaluation.html">
     Model Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../introduction/machinelearning/modelselection.html">
     Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../introduction/machinelearning/costsensitivelearning.html">
     Cost-Sensitive Learning
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Fairness
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Algorithmic Fairness
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="groupfairnessmetrics.html">
   Group Fairness Metrics
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="fairml/introduction.html">
   Fairness-Aware Machine Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="fairml/preprocessing.html">
     Pre-processing Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fairml/constrainedlearning.html">
     Constrained Learning Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fairml/postprocessing.html">
     Post-processing Algorithms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="interdisciplinary/introduction.html">
   Interdisciplinary Perspectives on Fair-ML
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="interdisciplinary/philosophy.html">
     Philosophy: What is “Fair”?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="interdisciplinary/law.html">
     Law: Fairness and Discrimination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="interdisciplinary/sts.html">
     Science and Technology Studies: Abstraction Traps
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="tutorials/introduction.html">
   Tutorials
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="tutorials/measuringgroupfairness.html">
     Measuring Group Fairness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tutorials/fairnessawareclassification.html">
     Fairness-Aware Classification
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Misc
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../misc/harms.html">
   List of Harms
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/hildeweerts/responsiblemachinelearning"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/hildeweerts/responsiblemachinelearning/issues/new?title=Issue%20on%20page%20%2Ffairness/introduction.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/fairness/introduction.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unfair-machines">
   Unfair Machines
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#types-of-harm">
   Types of Harm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#biases">
   Biases
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-bias">
     What is Bias?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#social-and-systemic-bias">
       Social and Systemic bias
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#statistical-bias">
       Statistical Bias
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#biases-in-a-machine-learning-development-process">
     Biases in a Machine Learning Development Process
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#historical-bias-social-biases-encoded-in-data">
       Historical Bias: Social Biases Encoded in Data
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#disparate-informativeness-representation-bias-and-predictive-bias">
       Disparate Informativeness: Representation Bias and Predictive Bias
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#aggregation-bias">
       Aggregation bias
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Algorithmic Fairness</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unfair-machines">
   Unfair Machines
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#types-of-harm">
   Types of Harm
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#biases">
   Biases
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-is-bias">
     What is Bias?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#social-and-systemic-bias">
       Social and Systemic bias
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#statistical-bias">
       Statistical Bias
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#biases-in-a-machine-learning-development-process">
     Biases in a Machine Learning Development Process
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#historical-bias-social-biases-encoded-in-data">
       Historical Bias: Social Biases Encoded in Data
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#disparate-informativeness-representation-bias-and-predictive-bias">
       Disparate Informativeness: Representation Bias and Predictive Bias
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#aggregation-bias">
       Aggregation bias
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="algorithmic-fairness">
<span id="intro-fairness"></span><h1>Algorithmic Fairness<a class="headerlink" href="#algorithmic-fairness" title="Permalink to this headline">#</a></h1>
<p>In recent years, there has been an increasing awareness amongst both the public and scientific community that algorithmic systems can reproduce, amplify, or even introduce unfairness or injustice in our societies. From automated resume screening tools that favor men over women to facial recognition systems that fail disproportionately for darker-skinned women. In this chapter, we provide an introduction to different types of fairness-related harms and the biases that cause them.</p>
<section id="unfair-machines">
<h2>Unfair Machines<a class="headerlink" href="#unfair-machines" title="Permalink to this headline">#</a></h2>
<p>Machine learning applications often make predictions about people. For example, algorithmic systems may be used to decide whether a resume makes it through the first selection round, judge the severity of a medical condition, or determine whether somebody will receive a loan. Since these systems are usually trained on large amounts of data, they have the potential to be more consistent than human decision-makers with varying levels of experience. For example, consider a resume screening process. In a non-automated scenario, the likelihood of getting through the resume selection round can depend on the personal beliefs of the recruiter who happens to judge your resume. On the other hand, the predictions of an algorithmic resume screening system can be learned from the collective judgment of many different recruiters.</p>
<p>However, the workings of a machine learning model heavily depend on how the machine learning task is formulated and which data is used to train the model. Consequently, prejudice and systemic bias against particular groups can seep into the model at each step of the development process. For example, if in the past a company has hired more men than women, this will be reflected in the training data. A machine learning model trained on this data is likely to pick up this pattern.</p>
<div class="admonition-example-amazon-s-resume-screening-model admonition">
<p class="admonition-title"><em>Example:</em> Amazon’s Resume Screening Model</p>
<p>In 2015, <a class="reference external" href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G">Amazon tried to train a resume screening model</a>. In order to avoid biased predictions, the model did not explicitly take into account the applicant’s gender. However, it turned out that the model penalized resumes that included terms that suggested that the applicant was a woman. For example, resumes that included the word “women’s” (e.g., in “women’s chess club captain”) were less likely to be selected. Although Amazon stressed the tool “was never used by Amazon recruiters to evaluate candidates”, this incident serves as an example of how machine learning models can, unintentially, replicate undesirable relationships between a sensitive characteristic and a target variable.</p>
</div>
<p>Notably, the characteristics that could potentially make algorithmic systems desirable over human-decision making, also amplify fairness-related risks. One prejudiced recruiter can judge a few dozen resumes each day, but an algorithmic system can process thousands of resumes in the blink of an eye. If an algorithmic system is biased in any way, harmful consequences will be structural and can occur at a large scale. Even if predictions do not directly consider individuals, people can be unfairly impacted <a class="footnote-reference brackets" href="#footcite-fairmlbook" id="id1">1</a>. For example, a machine learning model that predicts house valuations can influence the materialized sale prices. If some neighborhoods receive much lower house price predictions than others, this may disproportionately affect some groups over others.</p>
<p>Discrimination and bias of algorithmic systems is not a new problem. Well over two decades ago, Friedman and Nissenbaum<a class="footnote-reference brackets" href="#footcite-friedman1996" id="id2">2</a> analyzed the fairness of computer systems. However, with the increasing use of algorithmic systems, it has become clear that the issue is far from solved. Researchers from a range of disciplines have started working on unraveling the mechanisms by which algorithmic systems can undermine fairness and how these risks can be mitigated.</p>
<p>This has given rise to the research field of <em>algorithmic fairness</em>: the idea that algorithmic systems should behave or treat people fairly, i.e., without discrimination on the grounds of sensitive characteristics such as age, sex, disability, ethnic or racial origin, religion or belief, or sexual orientation. Here, <em>sensitive characteristic</em> refers to a characteristic of an individual such that any decisions based on this characteristic are considered undesirable from an ethical or legal point of view.</p>
<p>Note that this definition of algorithmic fairness is very broad. This is intentional. The concept applies to all types of algorithmic systems, including different flavors of artificial intelligence (e.g., symbolic approaches, expert systems, and machine learning), but also simple rule-based systems.</p>
</section>
<section id="types-of-harm">
<span id="id3"></span><h2>Types of Harm<a class="headerlink" href="#types-of-harm" title="Permalink to this headline">#</a></h2>
<p>The exact meaning of “behaving or treating people fairly” depends heavily on the context of the algorithmic system. There are several different ways in which algorithmic systems can disregard fairness. In particular, we can distinguish between the following types of fairness-related harms<a class="footnote-reference brackets" href="#footcite-madaio2020" id="id4">3</a>.</p>
<a class="reference external image-reference" href="none"><img alt="A process model of a machine learning development process, with a chain of blocks starting at &quot;problem formulation&quot;, followed by &quot;collect data&quot;, &quot;train model&quot;, &quot;make predictions&quot;, &quot;make decisions&quot;, and finally &quot;impact&quot;." class="align-center" src="../_images/fairnessharms.svg" width="600px" /></a>
<ul class="simple">
<li><p><em>Allocation harm</em> can be defined as an unfair allocation of opportunities, resources, or information. In our resume selection example, allocation harm can occur when some groups are selected less often than others, e.g., the algorithm selects men more often than women.</p></li>
<li><p><em>Quality-of-service harm</em> occurs when a system disproportionately fails for certain (groups of) people. For example, a facial recognition system may misclassify black women at a higher rate than white men <a class="footnote-reference brackets" href="#footcite-buolamwini2018" id="id5">4</a> and a speech recognition system may not work well for users whose disability impacts their clarity of speech <a class="footnote-reference brackets" href="#footcite-guo2019" id="id6">5</a>.</p></li>
<li><p><em>Stereotyping harm</em> occurs when a system reinforces undesirable and unfair societal stereotypes. Stereotyping harms are particularly prevalent in systems that rely on unstructured data, such as natural language processing and computer vision systems. The reason for this is that societal stereotypes are often deeply embedded in text corpora and image labels. For example, an image search for “CEO” may primarily show photos of white men.</p></li>
<li><p><em>Denigration harm</em> refers to situations in which algorithmic systems are actively derogatory or offensive. For example, an automated tagging system may <a class="reference external" href="https://www.theverge.com/2015/7/1/8880363/google-apologizes-photos-app-tags-two-black-people-gorillas">misclassify people as animals</a> and a chatbot might <a class="reference external" href="https://fortune.com/2020/09/29/artificial-intelligence-openai-gpt3-toxic/">start using derogatory slurs</a>.</p></li>
<li><p><em>Representation harm</em> occurs when the development and usage of algorithmic systems over- or under-represents certain groups of people. For example, some racial groups may be overly scrutinized during welfare fraud investigations or neighborhoods with a high elderly population may be ignored because data on disturbances in the public space (such as <a class="reference external" href="https://hbr.org/2013/04/the-hidden-biases-in-big-data">potholes</a>) is collected using a smartphone app. Representation harm can be connected to allocation harms and quality-of-service harms.</p></li>
<li><p><em>Procedural harm</em> occurs when decisions are made in a way that violates social norms (see e.g., Rudin <em>et al.</em><a class="footnote-reference brackets" href="#footcite-rudin2018a" id="id7">6</a>). For example, penalizing a job applicant for having more experience can violate social norms. Procedural harm is not limited to the prediction-generating mechanisms of the machine learning model but can be extended to the development and usage of the system. For example, is it communicated clearly that an algorithmic decision is made? Do data subjects receive a meaningful justification? Is it possible to appeal a decision?</p></li>
</ul>
<p>Note that these types of harm are not mutually exclusive. For example, over- or underrepresentation of particular groups can influence the predictive performance of the model, resulting in quality-of-service harm. Additionally, this list is not complete – there may be other context and application-specific harms.</p>
</section>
<section id="biases">
<h2>Biases<a class="headerlink" href="#biases" title="Permalink to this headline">#</a></h2>
<p>Anybody who has ever attempted to build a machine learning model will know that machine learning systems are an accumulation of design choices that shape what the final model will look like. From high-level decisions related to the goal of the model to a plethora of detailed choices. Will you use one-hot-encoding for that categorical feature in your data set? Which hyperparameter settings are you going to evaluate? Which data is deemed of sufficiently high quality to train the model? These puzzles are part of what makes machine learning development interesting and creative work. However, choosing the <em>‘best’</em> option implicitly involves a value judgment. This has important implications. In the context of algorithmic fairness, a major concern revolves around different forms of biases that can seep into a machine learning system through various design choices. In this section, we will explore several of these biases as sources of fairness-related harm.</p>
<section id="what-is-bias">
<h3>What is Bias?<a class="headerlink" href="#what-is-bias" title="Permalink to this headline">#</a></h3>
<p>But first, let us define more precisely what we mean when we talk about ‘bias’. In the dictionary, bias is defined as “a systematic and disproportionate tendency towards something”. It can refer to many different things, ranging from social biases related to prejudice to statistical biases that are technical.</p>
<a class="reference external image-reference" href="none"><img alt="A process model of a machine learning development process, with a chain of blocks starting at &quot;problem formulation&quot;, followed by &quot;collect data&quot;, &quot;train model&quot;, &quot;make predictions&quot;, &quot;make decisions&quot;, and finally &quot;impact&quot;." class="align-center" src="../_images/bias.svg" width="300px" /></a>
<section id="social-and-systemic-bias">
<h4>Social and Systemic bias<a class="headerlink" href="#social-and-systemic-bias" title="Permalink to this headline">#</a></h4>
<p>In everyday language, bias is typically used to refer to prejudice against a person or a group. To avoid confusion, we will generally refer to this type of bias as social bias. Social bias is a form of cognitive bias: a systematic error in rational thinking that can affect judgment and decision-making. Several theoretical explanations of cognitive biases exist. In particular, the biases are thought to be the result of the limitations of the information-processing capabilities of the human brain. From an evolutionary perspective, cognitive biases might have been useful because they allow people to make quick decisions in critical scenarios. As a hunter-gatherer, you would probably rather be safe than sorry when encountering an unknown group of other humans. However, shortcuts and heuristics often come at the cost of the quality of decisions. In particular, stereotypes formed by social bias can be overgeneralized and inaccurate, especially on an individual level. When people act on social biases, they can result in discriminatory practices. Social bias is not limited to human actors and can also be embedded in institutions, in which case it is typically referred to as systemic bias or institutional bias.</p>
</section>
<section id="statistical-bias">
<h4>Statistical Bias<a class="headerlink" href="#statistical-bias" title="Permalink to this headline">#</a></h4>
<p>If you have ever done an introductory course on statistics or machine learning theory, you have almost certainly come across statistical bias. In this context, ‘bias’ refers to a systematic error in the estimation of parameters or variables. For example, you may have come across a <a class="reference external" href="https://en.wikipedia.org/wiki/Variance#Sample_variance">naive estimation of sample variance</a> in one of your statistics classes, which is a provably biased estimate of the true variance of a population. In other cases, statistical bias is used to refer to biases in the data collection process that compromise the accuracy of an estimate. For example, respondents of a Twitter poll are rarely a random sample of the population. Bias is also used to refer to systematic errors caused by assumptions of an estimator. In the context of machine learning algorithms, this type of bias is often discussed with the bias-variance trade-off. For example, some machine learning algorithms can only learn linear relationships between features, whereas the true underlying data distribution exhibits more complex relationships, resulting in an underfitting model.</p>
</section>
</section>
<section id="biases-in-a-machine-learning-development-process">
<h3>Biases in a Machine Learning Development Process<a class="headerlink" href="#biases-in-a-machine-learning-development-process" title="Permalink to this headline">#</a></h3>
<p>Many different types of bias can result in fairness-related harm. Most of these issues arise at the intersections of social or systemic and statistical bias. In the remainder of this section, we will dive more deeply into different types of biases during data collection and modeling.</p>
<a class="reference external image-reference" href="none"><img alt="A process model of a machine learning development process, with a chain of blocks starting at &quot;problem formulation&quot;, followed by &quot;collect data&quot;, &quot;train model&quot;, &quot;make predictions&quot;, &quot;make decisions&quot;, and finally &quot;impact&quot;." class="align-center" src="../_images/process.svg" width="500px" /></a>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The list of biases discussed in this chapter is by no means exhaustive. Moreover, reality is messy, and in practice biases are much harder to precisely dissect than suggested in research papers. In the algorithmic fairness literature, several of these biases are known under various different names.</p>
</div>
<section id="historical-bias-social-biases-encoded-in-data">
<span id="historical-bias"></span><h4>Historical Bias: Social Biases Encoded in Data<a class="headerlink" href="#historical-bias-social-biases-encoded-in-data" title="Permalink to this headline">#</a></h4>
<p>When it comes to bias, one stage of the development process is arguably the most notorious: data collection and processing. While <em>bias in = bias out</em> only scratches the surface of fairness issues (more on that later), data sets are important sources of fairness-related harm.</p>
<p>One of the seemingly most obvious ways in which data can be biased is when social biases are explicitly encoded in data, typically in the form of an association between a sensitive feature and the target variable. If not accounted for, a machine learning model will reproduce these biases, resulting in unfair outcomes. Generally speaking, <em>historical bias</em> comes in two flavors.</p>
<a class="reference external image-reference" href="none"><img alt="A graphical representation of different types of historical bias. The picture contains three primary objects; 'potential - one's inate potential to become a data scientist', 'construct - employee quality', and 'measurement - hiring decisions'. An arrow is drawn between 'potential' and 'construct', which is annotated with '(un)just life bias, innate potential is not equal to employee quality'. A second arrow is drawn between 'construct' and 'measurement', which is annotated with 'measurement bias, employee quality is not equal to hiring decisions'." class="align-center" src="../_images/historicalbias.svg" width="600px" /></a>
<p>First, historical bias can arise due to social or statistical biases in historical decision-making, resulting in <em>measurement bias</em>. Generally speaking, measurement bias occurs when the method of observation results in systematic errors. When measurement bias is related to a sensitive feature, such as gender or race, it can be a source of downstream model unfairness. For example, a company may have historically hired more men than women for technical positions due to social bias. In this case, historical hiring decisions are a biased proxy for the actual suitability of the applicant. A model trained on historical decisions will likely reproduce the association. Another example can be found in fraud detection. A fraud analyst might overly scrutinize some groups over others. Higher rates of testing will result in more positives, confirming the analysts biased beliefs and skewing the observed base rates. Unaccounted for, the skewed numbers will be reproduced by the machine learning model. Inaccurate stereotypes can also be embedded in texts, images, and annotations produced by people, resulting in systems that reinforce these stereotypes. It is important to emphasize that simply removing the sensitive feature is unlikely to mitigate this type of bias: a sufficiently accurate model will simply reproduce the association.</p>
<p>A second type of historical bias occurs when the data is a good representation of reality, but reality is biased, which we will refer to as <em>life’s bias</em>. An alternative explanation of the observed historical bias in the hiring example is actual differences in suitability caused by structural inequalities in society. For example, people from lower socioeconomic backgrounds may have had fewer opportunities to get a good education, making them less suitable for jobs where such education is required for job performance. Similarly, while stereotypes can be very inaccurate at an individual level, social and cultural factors can influence how we organize our societies. For example, many occupations are highly gendered, resulting in differences in applicant pools.</p>
<p>In practice, it is impossible to distinguish between these two types of historical bias from observed data alone. For example, we cannot know the true underlying fraud rate if we only investigate data produced by a biased fraud detection methodology. Moreover, when social biases are involved, cases of measurement bias and structural inequality often co-occur. However, the distinction is important: while measurement bias can be mitigated by thoughtful and high-quality data collection, meaningfully addressing structural injustice in the context of a single machine learning model may not be feasible.</p>
</section>
<section id="disparate-informativeness-representation-bias-and-predictive-bias">
<span id="disparate-informativeness"></span><h4>Disparate Informativeness: Representation Bias and Predictive Bias<a class="headerlink" href="#disparate-informativeness-representation-bias-and-predictive-bias" title="Permalink to this headline">#</a></h4>
<p>A different set of data biases are related to the extent to which the data is sufficiently informative to make accurate predictions for all sensitive groups.</p>
<p>A data set can be less informative for a sensitive group if fewer data points are available. In particular, <em>representation bias</em> occurs when some groups are underrepresented in the data <a class="footnote-reference brackets" href="#footcite-suresh2020" id="id8">7</a>. A machine learning model might not generalize well for underrepresented groups, causing allocation harm or quality-of-service harm. Representation bias is especially risky when the data distribution of minority groups differs substantially from the majority group. An often-cited example of representation bias was uncovered by Buolamwini and Gebru<a class="footnote-reference brackets" href="#footcite-buolamwini2018" id="id9">4</a>. They found that the data sets that were used to train commercial facial recognition systems contained predominantly images of white men. Consequently, the models did not generalize well to people with dark skin, especially women.</p>
<p>Representation bias is closely related to selection bias, a statistical bias that occurs when the data collection or selection results in a non-random sample of the population. If not taken into account, conclusions regarding the studied effect may be wrong. For example, young healthy people may be more likely to volunteer for a vaccine trial than less healthy older people. As a result, conclusions about the side effects may not be representative of the whole population. Notably, representation bias can occur even when a sample is truly selected at random, as there may not be sufficient information available for minority groups.</p>
<p>An underlying cause of representation bias are blind spots of the collectors. For example, a data science team that consists solely of women is less likely to notice that men are not well represented in the data than a more diverse team. Additionally, some data is easier to get than others. For example, collecting data on the interests of young adults, who often spend hours each day scrolling through their social media feeds, is much easier compared to the interests of elderly people who are generally not as active online.</p>
<p>The informativeness of the data can also be affected by <em>predictive bias</em>, which occurs when the informativeness of the features differs across groups. In this case, it is not so much the amount of data that is relevant, but the quality of that data for a predictive task. For example, women face different risk factors for heart disease compared to men, including pregnancy and menopause-induced factors. If we were to build a machine learning model that does not take into account these features, the model is likely to be less accurate for women than it is for men.</p>
<p>Representation bias is relatively easy to solve by getting more data. Additionally, data scientists can leverage techniques that were designed to deal with sampling errors, such as weighting instances. However, one should be cautious to not overburden already marginalized groups. Moreover, it is hard to justify harming other values, such as privacy and autonomy, in your quest for more data. For example, after IBM released its “diverse data set”, it became clear that the data set <a class="reference external" href="https://www.nbcnews.com/tech/internet/facial-recognition-s-dirty-little-secret-millions-online-photos-scraped-n981921">contained millions of photos that were retrieved from the internet</a> without consent. Similarly, researchers attempting to increase the predictive performance of facial recognition systems for transgender persons, proceeded to <a class="reference external" href="https://www.theverge.com/2017/8/22/16180080/transgender-youtubers-ai-facial-recognition-dataset">collect a set of Youtube video links of transgender individuals</a>, without informing them. Predictive bias, on the other hand, cannot be solved by collecting more data points, only by collecting better data. For example, we may carefully consider which additional features should be added to the model by consulting domain experts.</p>
</section>
<section id="aggregation-bias">
<span id="id10"></span><h4>Aggregation bias<a class="headerlink" href="#aggregation-bias" title="Permalink to this headline">#</a></h4>
<p>Building a machine learning model includes many different choices, ranging from the class of machine learning algorithms that are considered to their hyperparameter settings. Different models may have different consequences related to fairness, depending on the task at hand.</p>
<p><em>Aggregation bias</em> occurs when a single model is used for groups that have distinct data distributions <a class="footnote-reference brackets" href="#footcite-suresh2020" id="id11">7</a>. If not accounted for, it may lead to a model that does not work well for any of the subgroups. For example, it is known that the relationship between hemoglobin levels (HbA1c) and blood glucose levels differs greatly across genders and ethnicities <a class="footnote-reference brackets" href="#footcite-suresh2020" id="id12">7</a>. If these differences are not taken into account, a model that only uses a single interpretation of HbA1c will likely not work well for any of these groups. In combination with representation bias, it can lead to a model that only works well for the majority population.</p>
<p>Aggregation bias is related to the problem of <em>underfitting</em>. Machine learning can be seen as a compression problem that produces a mapping between input features and output variables. Some information is inherently lost because of the chosen mapping <a class="footnote-reference brackets" href="#footcite-dobbe2018" id="id13">8</a>. In particular, some model classes may not be able to adequately capture the different data distributions. Such an oversimplified model may come at the cost of predictive performance for minority groups.</p>
<div class="tip admonition">
<p class="admonition-title">Summary</p>
<p><strong>Unfair Machines</strong></p>
<p>Machine learning models can reproduce, amplify, and even introduce unfairness. This has given rise to the research field of algorithmic fairness: the idea that algorithmic systems should behave or treat people without discrimination on the grounds of sensitive characteristics.</p>
<p><strong>Types of harm</strong></p>
<p>There are different ways in which algorithmic systems can disregard fairenss, including allocation harm, quality-of-service harm, stereotyping harm, denigration harm, representation harm, and procedural harm.</p>
<p><strong>Biases as Sources of Unfairness</strong></p>
<p>Social, systemic, and statistical biases can influence the machine learning development process, causing fairness-related harm.</p>
<ul class="simple">
<li><p>Social biases can be encoded in data sets in the form of measurement bias and structural inequalities in society.</p></li>
<li><p>Representation bias and predictive bias affect the informativeness of data across different sensitive groups, which can result in disparate predictive performance.</p></li>
<li><p>During modelling, aggregation bias can result in a model that does not work well for sensitive groups that have distinct data distributions.</p></li>
</ul>
</div>
</section>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<div class="docutils container" id="id14">
<dl class="footnote brackets">
<dt class="label" id="footcite-fairmlbook"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Solon Barocas, Moritz Hardt, and Menaka Narayanan. Fairness and machine learning: limitations and opportunities. 2022. Retrieved from <a class="reference external" href="https://fairmlbook.org">https://fairmlbook.org</a>.</p>
</dd>
<dt class="label" id="footcite-friedman1996"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Batya Friedman and Helen Nissenbaum. Bias in computer systems. <em>ACM Transactions on Information Systems</em>, 14(3):330–347, July 1996. URL: <a class="reference external" href="https://doi.org/10.1145/230538.230561">https://doi.org/10.1145/230538.230561</a>, <a class="reference external" href="https://doi.org/10.1145/230538.230561">doi:10.1145/230538.230561</a>.</p>
</dd>
<dt class="label" id="footcite-madaio2020"><span class="brackets"><a class="fn-backref" href="#id4">3</a></span></dt>
<dd><p>Michael A Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI. <em>Chi 2020</em>, pages 1–14, 2020.</p>
</dd>
<dt class="label" id="footcite-buolamwini2018"><span class="brackets">4</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id9">2</a>)</span></dt>
<dd><p>Joy Buolamwini and Timnit Gebru. Gender shades: intersectional accuracy disparities in commercial gender classification. In <em>Conference on fairness, accountability and transparency</em>, 77–91. 2018.</p>
</dd>
<dt class="label" id="footcite-guo2019"><span class="brackets"><a class="fn-backref" href="#id6">5</a></span></dt>
<dd><p>Anhong Guo, Ece Kamar, Jennifer Wortman Vaughan, Hanna M. Wallach, and Meredith Ringel Morris. Toward fairness in AI for people with disabilities: A research roadmap. <em>CoRR</em>, 2019. URL: <a class="reference external" href="http://arxiv.org/abs/1907.02227">http://arxiv.org/abs/1907.02227</a>, <a class="reference external" href="https://arxiv.org/abs/1907.02227">arXiv:1907.02227</a>.</p>
</dd>
<dt class="label" id="footcite-rudin2018a"><span class="brackets"><a class="fn-backref" href="#id7">6</a></span></dt>
<dd><p>Cynthia Rudin, Caroline Wang, and Beau Coker. The age of secrecy and unfairness in recidivism prediction. <em>Harvard Data Science Review</em>, 3 2020. https://hdsr.mitpress.mit.edu/pub/7z10o269. URL: <a class="reference external" href="https://hdsr.mitpress.mit.edu/pub/7z10o269">https://hdsr.mitpress.mit.edu/pub/7z10o269</a>, <a class="reference external" href="https://doi.org/10.1162/99608f92.6ed64b30">doi:10.1162/99608f92.6ed64b30</a>.</p>
</dd>
<dt class="label" id="footcite-suresh2020"><span class="brackets">7</span><span class="fn-backref">(<a href="#id8">1</a>,<a href="#id11">2</a>,<a href="#id12">3</a>)</span></dt>
<dd><p>Harini Suresh and John V. Guttag. A framework for understanding unintended consequences of machine learning. 2020. <a class="reference external" href="https://arxiv.org/abs/1901.10002">arXiv:1901.10002</a>.</p>
</dd>
<dt class="label" id="footcite-dobbe2018"><span class="brackets"><a class="fn-backref" href="#id13">8</a></span></dt>
<dd><p>Roel Dobbe, Sarah Dean, Thomas Gilbert, and Nitin Kohli. A broader view on bias in automated decision-making: reflecting on epistemology and dynamics. <em>arXiv preprint arXiv:1807.00553</em>, 2018.</p>
</dd>
</dl>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./fairness"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../introduction/machinelearning/costsensitivelearning.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Cost-Sensitive Learning</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="groupfairnessmetrics.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Group Fairness Metrics</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Hilde Weerts<br/>
  
      &copy; Copyright 2024.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>